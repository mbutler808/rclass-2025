{
  "hash": "11d0447aa565caa7ac8aaf3262a647b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data IO\"\nauthor:\n  - name: Marguerite Butler\n    url: https://butlerlab.org\n    affiliation: School of Life Sciences, University of Hawaii\n    affiliation_url: https://manoa.hawaii.edu/lifesciences/\ndescription: \"So many ways to get data into R\"\ndate: 2025-02-04\ncategories: [module 2, week 3, data, input, output, formats]\nbibliography: bibliography.bib\nformat:\n  html:\n    code-fold: false\n---\n\n\n<!-- Add interesting quote -->\n\n### Acknowledgements\n\nMaterial for this lecture was borrowed and adopted from\n\n-  <http://rafalab.dfci.harvard.edu/dsbook/importing-data.html>\n-  <https://www.stephaniehicks.com/jhustatcomputing2022>\n\n\n# Learning objectives\n\n::: callout-note\n# Learning objectives\n\n**At the end of this lesson you will:**\n\n-   Be able to read and write text / csv files in R\n-   Diagnose some common problems importing data and troubleshoot\n-   Be able to identify different file types and encodings\n-   Plan using best practices for data management and spreasheet design\n-   Be able to calculate memory requirements for R objects\n-   Be aware of various packages to read in other types of data\n:::\n\n# Introduction\n\n<!-- Sweave2knitr() -->\n\nSo far we have either typed data in directly into R or used R functions to generate data. In order to analyze your own data, you have to load data from an external file into R.  Similarly, to save your work, you'll want to write files to your hard drive. Both of these require interacting with your computer's operating system. In this chapter, we'll practice doing it, so you learn the mechanics. Please be patient and we'll talk more about what's going on in the next lesson on scripts and the R Environment.\n\n::: callout-note\n## There are many different ways to access data in R\n\n1.  Built-in datasets - useful for practice\n2.  Text files, importing from .csv is especially easy\n3.  Files in other formats, e.g., .xls\n4.  readLines from a connection including files\n5.  Data not on your local computer, e.g., piped via a url\n6.  Special connections, e.g., google sheets\n:::\n\n# Built-in datasets\n\nR has many built-in datasets that come distributed with R. They are very useful for demonstrating Rʻs many functions for statistics, plotting, and other uses.\n\nTo see a list of them, you can type `data()` at the command line. To see the contents, type the name of the dataset. To see the help page type a ? in front of the name:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata()\ncars\n?cars\n```\n:::\n\n\n\nYou can use built-in data in R by referring to the names of the objects.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(cars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  speed dist\n1     4    2\n2     4   10\n3     7    4\n4     7   22\n5     8   16\n6     9   10\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(cars)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n# The biggest issue for fairly new users\n\nIn order for R to read in data, it needs to know (1) where it the data stream, and (2) what in the datastream it is looking for. In the case of built-in datasets, both problems are solved as it is already imported into R.\n\nThe biggest issue most new-ish users experience when the path or filename is incorrect. For example, I donʻt have a file called \"dat.csv\" here, so this is a common error:\n\n```{.bash filename=R}\n> read.csv(\"dat.csv\")\nError in file(file, \"rt\") : cannot open the connection\nIn addition: Warning message:\nIn file(file, \"rt\") : cannot open file 'dat.csv': No such file or directory\n```\n\nThe `cannot open the conection` is a pipe or path error. It cannot open a pipe or a connection to the file.  `No such file or directory` is just that. R doesnʻt know where the file is. Is there a typo? Is it the correct path?\n\nOnce we are in the correct working directory and fixed all typos, most of the time we are good, but we may have some surprises in the file format which weʻll go into below.\n\nThe issue is basically that R is parsing the information stream as it reads the file and filtering what counts as data, and what is not data (also called \"delimeters\" or the things that separate your data).  Once you understand how to specify delimeters, you are in great shape!\n\n# Reading in Text Files\n\nThe most convenient way to read data into R is using the `read.csv()` function. This requires that your data is saved in `.csv` format, which is possible from Microsoft Excel (save as... csv) or any spreadsheet format. It is a text format with data separated by commas. (Open it in a plain text editor and take a look).\n\nIt is very nice because it is unambiguous, not easily corruptible, and non-proprietary. Thus it is readable by nearly every program that reads in data.\n\n## Get the files onto your computer\n\nFirst, get the data files from the [`rclassdata`](https://github.com/mbutler808/rclassdata) GitHub repo by cloning the repo to your `Documents/git/rclass` folder. \n\nOpen R by double clicking on an R script within your `rclassdata` folder. Which working directory are you in? If necessary, use `setwd()` to get move into the `rclassdata` folder.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetwd()\nsetwd(\"~/Documents/git/rclass/rclassdata\")  \nlist.files()  # will tell us which files are in this folder\n```\n:::\n\n\n\n## read.csv\n\nGetting the file into R is easy. If it is in csv format, you just use:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread.csv(\"anolisSSD.csv\")  # look for the file in the working directory\n```\n:::\n\n\n\nThis is an *Anolis* lizard sexual size dimorphism dataset. It has values of dimorphism by species for different ecomorphs, or microhabitat specialists.\n\nTo save the data as an R object, give it a name and save it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanolis <- read.csv(\"anolisSSD.csv\")\n```\n:::\n\n\n\nIt is a good practice to *always* check that the data were read in properly. If it is a large file, you'll want to at least check the beginning and end were read in properly:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(anolis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  species   logSSD    ecomorph\n1      oc -0.00512        twig\n2      eq  0.08454 crown-giant\n3      co  0.24703 trunk-crown\n4     aln  0.24837 trunk-crown\n5      ol  0.09844  grass-bush\n6      in  0.06137        twig\n```\n\n\n:::\n\n```{.r .cell-code}\ntail(anolis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   species  logSSD     ecomorph\n18      cr 0.39796 trunk-ground\n19      st 0.15737  trunk-crown\n20      cy 0.26024 trunk-ground\n21     alu 0.08216   grass-bush\n22      lo 0.13108        trunk\n23      an 0.13547         twig\n```\n\n\n:::\n:::\n\n\n\nWhich prints out the first six and last six lines of the file.\n\nVoila! Now you can plot, take the mean, etc.  For example: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(as.factor(anolis$ecomorph), anolis$logSSD)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n(Aside) You probably didn't understand the line of code above, but we will get into this very soon in the next lectures. But you can break it down bit by bit. \n\nNow try reading in `anolisSSDsemicolon.csv`  what did you get? Try reading it in with `read.table()` - check out the help page.\n\nR can read in many other formats as well, including database formats, excel native format (although it is easier in practice to save as .csv), fixed width formats, and scanning lines. For more information see the R manual \"R Data Import/Export\" which you can get from `help.start()` or at <http://www.r-project.org>.\n\n\n## Input files generated by data loggers\n\nFiles that are generated by computer, even if they are not separated by commas (.csv) are not too bad to deal with. Take, for example, the file format generated from our hand-held Ocean Optics specroradiometer. It is very regular in structure, and we have tons of data files, so it is well worth the programming effort to code a script for automatic file input.\n\nFirst, you can open the file below in a text editor. If you'd rather open it in R, you can use:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreadLines(\"20070725_01forirr.txt\")\n```\n:::\n\n\n\nNotice that there is a very large header, in fact the first 17 lines. Notice also that the last line will cause a problem. Also, the delimiter in this file is tab (backslash t).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <- readLines(\"20070725_01forirr.txt\")\nhead(temp)\ntail(temp)\n```\n:::\n\n\n\nWe can solve these issues using the `skip` and the `comment.char` arguments of `read.table` to ignore both types of lines, reading in only the \"good stuff\". Also, the default delimiter in this function is the tab:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- read.table(file=\"20070725_01forirr.txt\", skip=17, comment.char=\">\")\nnames(dat) <- c(\"lambda\", \"intensity\")\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lambda intensity\n1 177.33         0\n2 177.55         0\n3 177.77         0\n4 177.99         0\n5 178.21         0\n6 178.43         0\n```\n\n\n:::\n\n```{.r .cell-code}\ntail(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     lambda intensity\n3643 888.21   0.29491\n3644 888.38   0.31306\n3645 888.54   0.28153\n3646 888.71   0.28245\n3647 888.87   0.18988\n3648 889.04   0.18988\n```\n\n\n:::\n:::\n\n\n\nThe file produces (useless) rows of data outside of the range of accuracy of the\nspectraradiometer. We can get rid of these by subsetting the data, selecting only the range 300-750nm:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- dat[dat$lambda >= 300, ]  # cut off rows below 300nm\ndat <- dat[dat$lambda <= 750, ]  #cut off rows above 750nm\n```\n:::\n\n\n\nOr do both at once:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- dat[dat$lambda >= 300 & dat$lambda <= 750,]\n```\n:::\n\n\n\nIf we are going to be doing this subsetting over and over, we might want to save this as an index vector which tells us the position of the rows of data we want to keep in the dataframe (don't worry, we'll cover this again in the workhorse functions chapter).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noo <- dat$lambda >= 300 & dat$lambda <= 750\ndat <- dat[oo, ]   # same as longer version above\n```\n:::\n\n\n\nWe can now save the cleaned up version of the irradiance data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite.csv(dat, \"20070725_01forirr.csv\")\n```\n:::\n\n\n\n# Read from URL\n\nA URL is just another datastream or pipe.  If it is a .csv file on the internet, R supports reading it in directly:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanolis2 <- read.csv(\"https://raw.githubusercontent.com/mbutler808/rclassdata/main/2023-01-31-DataIO/anolisSSD.csv\")\nhead(anolis2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  species   logSSD    ecomorph\n1      oc -0.00512        twig\n2      eq  0.08454 crown-giant\n3      co  0.24703 trunk-crown\n4     aln  0.24837 trunk-crown\n5      ol  0.09844  grass-bush\n6      in  0.06137        twig\n```\n\n\n:::\n\n```{.r .cell-code}\ncbind(anolis, anolis2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   species   logSSD     ecomorph species   logSSD     ecomorph\n1       oc -0.00512         twig      oc -0.00512         twig\n2       eq  0.08454  crown-giant      eq  0.08454  crown-giant\n3       co  0.24703  trunk-crown      co  0.24703  trunk-crown\n4      aln  0.24837  trunk-crown     aln  0.24837  trunk-crown\n5       ol  0.09844   grass-bush      ol  0.09844   grass-bush\n6       in  0.06137         twig      in  0.06137         twig\n7       cu  0.09501  crown-giant      cu  0.09501  crown-giant\n8       ri  0.08947  crown-giant      ri  0.08947  crown-giant\n9       sa  0.28893 trunk-ground      sa  0.28893 trunk-ground\n10      op  0.15963   grass-bush      op  0.15963   grass-bush\n11      va  0.14766         twig      va  0.14766         twig\n12      li  0.38873 trunk-ground      li  0.38873 trunk-ground\n13      ga  0.28768  crown-giant      ga  0.28768  crown-giant\n14      gr  0.39786  trunk-crown      gr  0.39786  trunk-crown\n15      br  0.17106        trunk      br  0.17106        trunk\n16      di  0.13801        trunk      di  0.13801        trunk\n17      kr  0.23478   grass-bush      kr  0.23478   grass-bush\n18      cr  0.39796 trunk-ground      cr  0.39796 trunk-ground\n19      st  0.15737  trunk-crown      st  0.15737  trunk-crown\n20      cy  0.26024 trunk-ground      cy  0.26024 trunk-ground\n21     alu  0.08216   grass-bush     alu  0.08216   grass-bush\n22      lo  0.13108        trunk      lo  0.13108        trunk\n23      an  0.13547         twig      an  0.13547         twig\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-tip}\n## Use the correct file format\n\nMake sure that your URL leads to a raw `.csv` file, and not the `.html` rendering of your csv file!\n:::\n\n# What to do when your spreadsheet is malformed\n\nYou will inevitably run into malformed spreadsheets. If there are unexpected characters used as delimiters, irregular delimiters, or uneven numbers of elements, these will all cause problems reading in data smoothly into any software.\n\nMost of the time, these issues are inadvertent. For example, if you cut and paste off the web, extra characters can easily be introduced. Try reading in `species_list_IUCN.csv`. It is a large spreadsheet of every species of subfamily Asteriorphinae frog in the world (over 330). Does it look OK?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <- read.csv(\"species_list_IUCN.csv\")\nhead(temp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                                                   SPECIES.OF.ASTEROPHRYINAE.REPRESENTED.IN.IUCN.SHAPEFILE.LIST.\n1   Aphantophryne minuta                                                                                                        \n2   Aphantophryne nana                                                                                                          \n3   Aphantophryne pansa                                                                                                         \n4   Aphantophryne parkeri                                                                                                       \n5   Aphantophryne sabini                                                                                                        \n6   Asterophrys eurydactyla                                                                                                     \n   X X.1 X.2 X.3\n1 NA  NA  NA  NA\n2 NA  NA  NA  NA\n3 NA  NA  NA  NA\n4 NA  NA  NA  NA\n5 NA  NA  NA  NA\n6 NA  NA  NA  NA\n```\n\n\n:::\n:::\n\n\n\nItʻs hard to tell from this output what the problem is. Try `readLines`, which will show you the entire line of data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntempL <- readLines(\"species_list_IUCN.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in readLines(\"species_list_IUCN.csv\"): incomplete final line found on\n'species_list_IUCN.csv'\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(tempL)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SPECIES OF ASTEROPHRYINAE REPRESENTED IN IUCN SHAPEFILE LIST:,,,,\"                                                                 \n[2] \"Aphantophryne minuta                                                                                                        ,,,,\"  \n[3] \"Aphantophryne nana                                                                                                          ,,,,\"  \n[4] \"  Aphantophryne pansa                                                                                                         ,,,,\"\n[5] \"  Aphantophryne parkeri                                                                                                       ,,,,\"\n[6] \"  Aphantophryne sabini                                                                                                        ,,,,\"\n```\n\n\n:::\n:::\n\n\n\nNow you can clearly see that it is extra whitespace. Extra blank columns, and a huge long line of white space after each species name. Starting from *Aphantophryne pansa*, there are two extra spaces in front of each species name.\n\nThe easiest thing to do here is to open up the spreadsheet in your spreadsheet program (or plain text editor) and read it back in. Delete the extra columns, and do search and replace on the two leading spaces and trailing multiple spaces. Also change that very long name to something more reasonable like \"gensp\". (This is an example of using variable names as annotations or comments - donʻt do it.)\n\nAlternatively, you can solve these problems within R. We will learn these object and string manipulation skills later, but here is one way to solve the problems once you have diagnosed them.\n\n#### Read in and select only the first columns\n\nThe rest of the columns are blank, so we really only need the first one. Also lets give it a shorter name.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- read.csv(\"species_list_IUCN.csv\")\ndat <- dat[1]   # select only the first column and resave dat\nnames(dat) <- \"gensp\"  # give the column a shorter name\n```\n:::\n\n\n\n#### Use function `trimws`\n\nR has a handy function `trimws` that trims white space\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngensp <- trimws(dat$gensp)\nhead(gensp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Aphantophryne minuta\"    \"Aphantophryne nana\"     \n[3] \"Aphantophryne pansa\"     \"Aphantophryne parkeri\"  \n[5] \"Aphantophryne sabini\"    \"Asterophrys eurydactyla\"\n```\n\n\n:::\n:::\n\n\n\nHow you deal with cleaning the data will depend on what the issue is. But the first step is always diagnosing the problem. Weʻll learn many more strategies as we go.\n\n\n# Entering data with spreadsheets\n\nData management is a major issue for any scientist, so it is worthwhile to develop some best practices. Entering data into spreadsheets is a fundamental step in any study using field data or any type of information that is gathered by humans. Some of the issues to consider are:\n\n1.  How can we best minimize human error?\n2.  What dataset structures are best for analyzing the data using software?\n3.  Compatability with other datasets in the lab, or commonly used in the field.\n\nIn the best case, we want to make our data available to the world, which means that it will have a permanence. We may also want to build on this data year after year, so it is worth thinking about a good design.\n\n## Human Error\n\nIt is very easy to make a typo, and humans are really bad at catching our own typos in real time. Right? They creep in despite our best efforts. This is why even though it is possible to measure specimens and enter the numbers directly into the computer, itʻs not something I would do.\n\nI always write my measurements down into a notebook or a datasheet using pencil and paper. This has saved me many errors in three ways (1) sometimes my brain is still processing what I just wrote and I will catch a typo. (2) I write my data in rows for specimens and columns for the different measurements. As the dataset builds, it is easy to notice errors if some number is really off. (3) If you have any doubts, you can quickly look over your page and re-measure anything suspicous. If you have the person-power, you can also have one person taking the measurements and calling them out, and another writing them down and repeating them back. It is a very effective way to check on the spot.\n\nI had my first post-college job at an insurance rating board. This is a business that deals with reams and reams of data. We did work a lot with Fortran code and spreadsheets, but surprisingly, some of it does actually have to get manually checked.\nThe protocol was simple, a paper printout of the old version was put next to the new version and a person went along with a ruler, literally putting a check mark after verifying the number. A second check was done as well, and then I finally understood the meaning of \"double checking\" 😝. When I got to grad school, it was eye-opening to find that checking any personʻs data entry was a rare practice 😲.  Please, whenever possible check your data entry. \n\n\n\n::: callout-tip\n## Here are some tips:\n\n1.  __Record data in a notebook__ (paper and pencil). It serves as a __permanent record__.\n2.  Write it down in a __data table format__ in an __order that minimizes error__. For example, if it is convenient to take five measurements in a particular order, then organize your table that way and always take the measurements in the same order.\n3.  __Organize your spreadsheet__ to _mirror_ the hand-written data. This will minimize data entry errors.\n4.  __Have another person check__ your data entry against the _notebook_ (data source).\n:::\n\n\n## Organizing your spreadhseet\n\nWhen it comes time to enter your data in a spreadsheet, there are many things you can do to improve organization. Below is a summary of the recommendations made in paper by Karl Broman and Kara Woo [@Broman:2018].\n\n-  **Be Consistent** - Have a plan before you start entering data. Be consistent and stick to it.\n-  **Choose Good Names for Things** - You want the names you pick for objects, files, and directories to be memorable, easy to spell, descriptive, but concise. This is actually a hard balance to achieve and it does require time and thought.\n    -  One important rule to follow is do not use spaces, use underscores _ or dashes instead -.\n    -  Also, avoid symbols; stick to letters and numbers.\n-  **Write Dates as YYYY-MM-DD** - To avoid confusion, we strongly recommend using this global ISO 8601 standard.\n-  **No Empty Cells** - Fill in all cells and use some common code for missing data.\n-  **Put Just One Thing in a Cell** - It is better to add columns to store the extra information rather than having more than one piece of information in one cell.\n-  **Make It a Rectangle** - The spreadsheet should be a rectangle.\n-  **Create a Data Dictionary** - If you need to explain things, such as what the columns are or what the labels used for categorical variables are, do this in a separate file. This is an excellent use for a README.md file\n-  **No Calculations in the Raw Data Files** - Excel permits you to perform calculations. Do not make this part of your spreadsheet. Code for calculations should be in a script.\n-  **Do Not Use Font Color or Highlighting as Data** - Most import functions are not able to import this information. Encode this information as a variable (a \"comment\" column) instead.\n-  **Make Backups** - Make regular backups of your data.\n-  **Use Data Validation to Avoid Errors** - Leverage the tools in your spreadsheet software so that the process is as error-free and repetitive-stress-injury-free as possible. Think of checks you can do for \"reality checks\".\n-  **Save the Data as Text Files** - Save files for sharing in comma or tab delimited format.  An unambiguous text format is the best for archiving your data.\n\n# A few words about Encodings\n\n## Text versus binary files\n\nFor data science purposes, files can generally be classified into two categories: text files (also known as **ASCII** files) and **binary** files. You have already worked with text files. All your R scripts are text files and so are the Quarto files used to create this website. The .csv tables you have read are also text files. One big advantage of these files is that we can easily “look” at them using a plain text editor, without having to purchase any kind of special software.\n\nAny text editor can be used to examine a text file, including freely available editors such as **R**, **RStudio**, **Atom**, **Notepad**, **TextEdit**, **vi**, **emacs**, **nano**, and **pico**. However, if you try to open, say, an Excel xls file, jpg or png file, you will not be able to see anything immediately useful. These are binary files. Excel files are actually compressed folders with several text files inside. But the main distinction here is that text files can be easily examined.\n\nAlthough R includes tools for reading widely used binary files, such as xls files, *in general you will want to find data sets stored in text files*. If necessary, use the proprietary software to export the data into text files and go from there.\n\n::: {.callout-warning}\nThe problem with using propietary formats for data management is that htey are poor formats for achival purposes. These formats may change or they donʻt copy well, so that eventually you can no longer open them.\n:::\n\nSimilarly, when sharing data you want to make it available as text files as long as storage is not an issue (binary files are much more efficient at saving space on your disk). In general, plain-text formats make it easier to share data since commercial software is not required for working with the data, and they are more reliable.\n\nTechnically, html and xml files are text files too, but they have complicated tags around the information. In the Data Wrangling part of the book we learn to extract data from more complex text files such as html files.\n\n## Unicode versus ASCII\n\nA pitfall in data science is assuming a file is an ASCII text file when, in fact, it is something else that can look a lot like an ASCII text file, for example, a Unicode text file.\n\nTo understand the difference between these, remember that everything on a computer needs to eventually be converted to 0s and 1s (binary format). ASCII is an encoding that maps bits to characters that are easier for humans to read.\n\n:::{.callout-note}\n# Binary data can take on only two values - 0 or 1\n-  a **bit** is the smallest unit, a single binary character (0 or 1).\n-  a **byte** is eight bits.\n-  a **megabyte** or MB is one million bytes.\n-  a **gigabyte** or GB is one billion bytes.\n-  You can roughly calculate the size of your data by the numbers of bytes per each observation.\n:::\n\nASCII uses 7 bits -- seven variables that can be either 0 or 1 -- which results in  2^7^ = 128 unique items, enough to encode all the characters on an English language keyboard (all characters, numbers, and symbols, @fig-ascii, @fig-ascii2). However, we need to expand the possibilities if we want to include support for other languages or additional characters.\n\n![Binary encodings in ACII, with some of the most used escape sequences like `\\t` for tab and `\\n` for new line.](ascii1.png){#fig-ascii}\n\n![Some examples of ASCII encodings for A-E and a-e.](ascii2.png){#fig-ascii2}\n\nFor this reason, a new encoding, using more than 7 bits, was defined: Unicode. When using Unicode, one can chose between 8, 16, and 32 bits abbreviated UTF-8, UTF-16, and UTF-32 respectively. RStudio actually defaults to UTF-8 encoding.\n\nAlthough we do not go into the details of how to deal with the different encodings here, it is important that you know these different encodings exist so that you can better diagnose a problem if you encounter it. One way problems manifest themselves is when you see “weird looking” characters you were not expecting.\n\nMany plain text editors (Atom, Sublime, TextWrangler, Notepad++) will detect encodings and tell you what they are and may also convert between them. Also from the command line the `file` command will reveal the encoding (should also work on Windows if you have `git` installed:\n\n```{.zsh filename=Terminal}\nfile filename\n```\n\nThis StackOverflow discussion is an example: <https://stackoverflow.com/questions/18789330/r-on-windows-character-encoding-hell>.\n\n\n## Line endings\n\nOne last potential headache is the character used for line endings. The line ending is the invisible (to us) character that is added to your file when you press the `return` key.\n\nIt is all one stream of informatin to the computer, but the computer will interpret the information as a new line when it detects one of these characters. When software is provided a file with an unexpected line ending, it is not able to properly detect the lines of information (maybe it sees only one huge line). See?\n\nThere are two types of line endings in use today:\n\n-  **On UNIX and MacOS**, text file line-endings are terminated with a newline character (ASCII 0x0a, represented by the `\\n` escape sequence in most languages), also referred to as a linefeed (LF).\n\n-  **On Windows**, line-endings are terminated with a combination of a carriage return (ASCII 0x0d or `\\r`) and a newline(`\\n`), also referred to as CR/LF.\n\nIf your computer complains about line endings, the easiest thing to do is to open it in one of the good plain text editors and save it with the line endings it is expecting (usually as LF instead of CR/LF).\n\n# Calculating Memory Requirements for R Objects\n\nBecause **R stores all of its objects in physical memory**, it is important to be aware of how much memory is being used up by all of the data objects residing in your workspace.\n\nIt is easy to make a back of the envelope calculation of how much memory will be required by a new dataset.  For example, suppose I have a data frame with 1,500,000 rows and 120 columns, all of which are numeric data. Roughly, how much memory is required to store this data frame?\n\nWell, on most modern computers [double precision floating point numbers](http://en.wikipedia.org/wiki/Double-precision_floating-point_format) are stored using 64 bits of memory, or 8 bytes. Given that information, you can do the following calculation\n\n1,500,000 × 120 × 8 bytes/numeric = 1,440,000,000 bytes\n\n= 1,440,000,000 bytes / 2^20^ bytes/MB\n\n= 1,373 MB\n\n= 1.37 GB\n\nSo the dataset would require about 1.37 GB of RAM. Most computers these days have at least that much RAM. However, you need to be aware of\n\n-   what other programs might be running on your computer, using up RAM\n-   what other R objects might already be taking up RAM in your workspace\n\nReading in a large dataset for which you do not have enough RAM is one easy way to freeze up your computer (or at least your R session). When that happens you to kill the R process, in the best case scenario, or reboot your computer, in the worst case. So before opening up potentially huge datasets,  make sure to do a rough calculation of memory requirements needed. You'll thank me later.\n\n*Also, itʻs aways good to build a script as you go and save frequently.*\n\n# Other data types\n\nNow, there are of course, many R packages that have been developed to read in all kinds of other datasets, and you may need to resort to one of these packages if you are working in a specific area.\n\nFor example, check out\n\n-   [`DBI`](https://github.com/r-dbi/DBI) for relational databases\n-   [`haven`](https://haven.tidyverse.org) for SPSS, Stata, and SAS data\n-   [`httr`](https://github.com/r-lib/httr) for web APIs\n-   [`readxl`](https://readxl.tidyverse.org) for `.xls` and `.xlsx` sheets\n-   [`googlesheets4`](https://googlesheets4.tidyverse.org) for Google Sheets\n-   [`googledrive`](https://googledrive.tidyverse.org) for Google Drive files\n-   [`rvest`](https://github.com/tidyverse/rvest) for web scraping\n-   [`jsonlite`](https://github.com/jeroen/jsonlite#jsonlite) for JSON\n-   [`xml2`](https://github.com/r-lib/xml2) for XML.\n\n# ASCII art\n\n[![Zebra in ASCII by Wilinckx, Wikimedia commons](https://upload.wikimedia.org/wikipedia/commons/4/45/BB-ASCII-art-screenshot-zebra.png)](https://nl.wikipedia.org/wiki/Gebruiker:Wilinckx)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}