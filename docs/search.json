[
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Tidying and Exploring Data\n\n\n\n\n\n\nmodule 2\n\n\nweek 5\n\n\ndata\n\n\ndata structures\n\n\nobjects\n\n\n\nAll about data and how it is represented in R\n\n\n\n\n\nFeb 18, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nTypes of Data\n\n\n\n\n\n\nmodule 2\n\n\nweek 4\n\n\ndata\n\n\ndata structures\n\n\nobjects\n\n\n\nAll about data and how it is represented in R\n\n\n\n\n\nFeb 13, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nSaving your work as R scripts\n\n\n\n\n\n\nmodule 1\n\n\nweek 4\n\n\nR\n\n\nscripts\n\n\nreproducibility\n\n\n\nSave your hard work as scripts!\n\n\n\n\n\nFeb 11, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nData IO\n\n\n\n\n\n\nmodule 2\n\n\nweek 3\n\n\ndata\n\n\ninput\n\n\noutput\n\n\nformats\n\n\n\nSo many ways to get data into R\n\n\n\n\n\nFeb 4, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nReference management\n\n\n\n\n\n\nmodule 1\n\n\nweek 3\n\n\nQuarto\n\n\nauthoring\n\n\nBibTeX\n\n\nprogramming\n\n\n\nHow to use citations and include your bibliography in R Quarto.\n\n\n\n\n\nJan 30, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nLiterate Statistical Programming and Quarto\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nMarkdown\n\n\nQuarto\n\n\nprogramming\n\n\n\nIntroduction to literate statistical programming tools including Quarto Markdown\n\n\n\n\n\nJan 28, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nR\n\n\nreproducibility\n\n\n\nIntroduction to reproducible research\n\n\n\n\n\nJan 23, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to git/GitHub\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nprogramming\n\n\nversion control\n\n\ngit\n\n\nGitHub\n\n\n\nVersion control is a game changer; or how I learned to love git/GitHub\n\n\n\n\n\nJan 21, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to your computerʻs terminal utilities\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nprogramming\n\n\nfilesystem\n\n\nshell\n\n\n\nSo much power; or how I got my computer to do my bidding\n\n\n\n\n\nJan 16, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction and The Big Idea\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nintroduction\n\n\n\nThe big idea\n\n\n\n\n\nJan 14, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Systems\n\n\n\n\n\n\nmodule 3\n\n\nweek 5\n\n\nplotting\n\n\nggplot2\n\n\nlattice\n\n\ndata visualization\n\n\n\nShowing you base plotting, lattice, and ggplot2\n\n\n\n\n\nFeb 20, 2024\n\n\nMarguerite Butler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-01-23-reproducible-research/index.html",
    "href": "posts/2025-01-23-reproducible-research/index.html",
    "title": "Reproducible Research",
    "section": "",
    "text": "The shocking assertion will be that most statistics in most scientific papers has errors. —Charles Geyer\n\nPre-lecture materials\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\n\nStatistical programming, Small mistakes, big impacts by Simon Schwab and Leonhard Held\n\nReproducibility and Error by Charles J. Geyer\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttp://users.stat.umn.edu/~geyer/Sweave/\nhttp://users.stat.umn.edu/~geyer/repro.pdf\nhttps://rdpeng.github.io/Biostat776\n\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nKnow the difference between replication and reproducibility\nIdentify valid reasons why replication and/or reproducibility is not always possible\nIdentify the type of reproducibility\nIdentify key components to enable reproducible data analyses\n\n\n\nIntroduction\nFrom a young age, we have learned that scientific conclusions should be reproducible. After all, isnʻt that what the methods section is for? We are taught to write methods sections so that any scientist could, in theory, repeat the experiment with the idea that if the phenomenon is true, they should obtain comparable results and more often than not should come to the same conclusions.\nBut how repeatable is modern science? Many experiments are now so complex and so expensive that repeating them is not practical. However, it is even worse than that. As datasets get larger and analyses become ever more complex, there is a growing concern that even given the data, we still cannot necessarily repeat the analysis. This is called “the reproducbility crisis”.\nRecently, there has been a lot of discussion of reproducibility in the media and in the scientific literature. The journal Science had a special issue on reproducibility and data replication.\n\nhttps://www.science.org/toc/science/334/6060\n\nTake for example a recent study by the Crowdsourced Replication Initiative (2022). It was a massive effort by 166 coauthors published in PNAS to test repeatability:\n\n73 research teams from around the world analyzed the same social science data.\nThey investigated the same hypothesis: that more immigration will reduce public support for government provision of social policies.\nTogether they fit 1261 statistical models and came to widely varying concluisons.\nA meta-analysis of the results by the PIs could not explain the variation in results. Even after accounting for the choices made by the research teams in designing their statistical tests, 95% of the total variation remained unexplained.\nThe authors claim that “a hidden universe of uncertainty remains.”\n\n\n\n\n\n\n\n\n\nSource: Breznau et al., 2022\nThis should be very disturbing. It was very disturbing to me! Greyer notes that the meta-analysis did not investigate how much of the variability of results was due to outright error. He furthermore notes that while the meta-analysis was done in a reproducibly, the original 73 analyses were not. What does he mean?\nSome of the issues from a statisticianʻs perspective\nGreyer provides nine ideas worth considering:\n\nMost scientific papers that need statistics have conclusions that are not actually supported by the statistical calculations done, because of\n\nmathematical or computational error,\nstatistical procedures inappropriate for the data, or\nstatistical procedures that do not lead to the inferences claimed.\n\n\nGood computing practices — version control, well thought out testing, code reviews, literate programming — are essential to correct computing.\nFailure to do all calculations from raw data to conclusions (every number or figure shown in a paper) in a way that is fully reproducible and available in a permanent public repository is, by itself, a questionable research practice.\nFailure to do statistics as if it could have been pre-registered is a questionable research practice.\nJournals that use P &lt; 0.05 as a criterion of publication are not scientific journals (publishing only one side of a story is as unscientific as it is possible to be).\nStatistics should be adequately described, at least in the supplementary material.\nScientific papers whose conclusions depend on nontrivial statistics should have statistical referees, and those referees should be heeded.\nNot all errors are describable by statistics. There is also what physicists call systematic error that is the same in every replication of an experiment. Physicists regularly attempt to quantify this. Others should too.\n\nA reasonable ideal for reproducible research today - Research should be reproducible. Anything in a scientific paper should be reproducible by the reader. - Whatever may have been the case in low tech days, this ideal has long gone. Much scientific research in recent years is too complicated and the published details to scanty for anyone to reproduce it. - The lack of detail is not entirely the author’s fault. Journals have severe page pressure and no room for full explanations. - For many years, the only hope of reproducibility is old-fashioned person-to-person contact. Write the authors, ask for data, code, whatever. Some authors help, some don’t. If the authors are not cooperative, tough. - Even cooperative authors may be unable to help. If too much time has gone by and their archiving was not systematic enough and if their software was unportable, there may be no way to recreate the analysis. - Fortunately, the internet comes to the rescue. No page pressure there! - Nowadays, many scientific papers also point to supplementary materials on the internet. Data, computer programs, whatever should be there, permanently. Ideally with a permanent Document Identifier or DOI. There are complaints that many Supplmentary Materials are incomprehensible, but that can be improved with practices of reproducible reserach.\nTherefore, at the very least scientists should use in their statistical programming - version control, - software testing, - code reviews, - literate programming, and - all data and code available in a permanent public repository.\nSome journals have specific policies to promote reproducibility in manuscripts that are published in their journals. For example, the Journal of American Statistical Association (JASA) requires authors to submit their code and data to reproduce their analyses and a set of Associate Editors of Reproducibility review those materials as part of the review process:\n\nhttps://jasa-acs.github.io/repro-guide\nRecommendations\nDiscuss Table 1\nAuthors and Readers\nIt is important to realize that there are multiple players when you talk about reproducibility–there are different types of parties that have different types of interests. There are authors who produce research and they want to make their research reproducible. There are also readers of research and they want to reproduce that work. Everyone needs tools to make their lives easier.\nOne current challenge is that authors of research have to undergo considerable effort to make their results available to a wide audience.\n\nPublishing data and code today is not necessarily a trivial task. Although there are a number of resources available now, that were not available even five years ago, it is still a bit of a challenge to get things out on the web (or at least distributed widely).\nResources like GitHub, kipoi, and RPubs and various data repositories have made a big difference, but there is still a ways to go with respect to building up the public reproducibility infrastructure.\n\nFurthermore, even when data and code are available, readers often have to download the data, download the code, and then they have to piece everything together, usually by hand. It’s not always an easy task to put the data and code together.\n\nReaders may not have the same computational resources that the original authors did.\nIf the original authors used an enormous computing cluster, for example, to do their analysis, the readers may not have that same enormous computing cluster at their disposal. It may be difficult for readers to reproduce the same results.\n\nGenerally, the toolbox for doing reproducible research is small, although it’s definitely growing.\n\nIn practice, authors often just throw things up on the web. There are journals and supplementary materials, but they are famously disorganized.\nThere are only a few central databases that authors can take advantage of to post their data and make it available. So if you are working in a field that has a central database that everyone uses, that is great. If you are not, then you have to assemble your own resources.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nThe process of conducting and disseminating research can be depicted as a “data science pipeline”\nReaders and consumers of data science research are typically not privy to the details of the data science pipeline\nOne view of reproducibility is that it gives research consumers partial access to the raw pipeline elements.\n\n\n\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy can replication be difficult to achieve? Why is reproducibility a reasonable minimum standard when replication is not possible?\nWhat is needed to reproduce the results of a data analysis?\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\n\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks"
  }
]