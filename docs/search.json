[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "For Qmd files (markdown document with Quarto which are cross-language executable code), go to the course GitHub repository and navigate the directories, or best of all clone the repo and navigate within RStudio or your text browser and R console.\nCurrent Website: https://mbutler808.github.io/rclass-2025/\n2023 Website: https://mbutler808.github.io/rclass/\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopics\nProjects\n\n\n\n\nModule 1\n\nComputational Tools for Data Science\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\nJan 14\n👋 Installing R/RStudio, GitHub [html] [Qmd]\n\nWatch [rest of podcast starting at 4:00] R manual [2.1, 2.2, 2.3, 2.6]\n\n\n\nJan 16\n👩‍💻 Shell scripts, File Organization [html] [Qmd]\n\n\n\n\nWeek 2\nJan 21\n🐙 Intro to Git and GitHub [html] [Qmd]\n🌴 Project 0 [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\nModule 2\n\nReproducible Research\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23\n🐙 Reproducible Research [html] [Qmd]\n\n\n\n\nWeek 3\nJan 28\n🐙 Literate Programming / Intro to Quarto [html] [Qmd]\n\n\n\n\n\nJan 30\n👓 Reference Management [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Analysis in R \n\n\n\n\n\n\n\n\n\n\n\n\nFeb 4\n👓 Reading/Writing data [html] [Qmd]\n🍂 Project 0 due\n\n\n\n\nFeb 6\n👓 Reading/Writing data continued [html] [Qmd]\n\n\n\n\nWeek 4\nFeb 11\n🐙 Scripts [html] [Qmd]\n\n\n\n\n\nFeb 13\n🔩 Data [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Visulization in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\nFeb 18\n🔪 Tidying Data [html] [Qmd]\n🌱 Project 1 assigned\n\n\n\n\nFeb 20\n📊 Plotting Systems (Base R) [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\nFeb 25\n📊 Plotting with ggplot2 [html] [Qmd]\n\n\n\n\n\nFeb 27\n📊 Tidying data with dplyr [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 4\n\nTour of Stats in R and Data Wrangling\n\n\n\n\nWeek 7\nMar 3\n\n🍂 Project 1 due\n\n\n\n\nMar 4\n🐒 Tidyverse and data wrangling [html] [Qmd]\n\n\n\n\n\nMar 6\n🐎 A tour of some univariate methods in R [html] [Qmd]\n🌱 Project 2 assigned\n\n\n\n\n\n\n\n\n\n\nWeek 8\nMar 11\n📆 A tour of some multivariate methods in R [html] [Qmd]\n\n\n\n\n\nMar 11\n🐎 Joining a.k.a. merging data [[html]] [[Qmd]]\n\n\n\n\n\nMar 13\n🐎 Reshaping data [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\nModule 5\n\nProgramming Elements\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\nMar 18\n🤝 Spring Break\n\n\n\n\n\nMar 20\n☀️ Spring Break\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\nMar 20\n🍄 Functions [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25\n🌵 Lists and For Loops [[html]] [[Qmd]]\n🍂 Project 2 due\n\n\n\n\n\n\n\n\n\n\n\nMar 27\n🌵 Apply Functions [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nApr 1\n🎡 Program Flow [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\nModule 6\n\nIntro to Morphometrics\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3\n📏 Intro to Morphometrics [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nApr 8\n📐 Landmark-based Morphometrics\n\n\n\n\n\n\n\n\n\n\n\n\nApr 10\n💀 3D Morphometrics\n🌰 Project 3 data check due\n\n\n\n\n\n\n\n\n\n\nModule 7\n\nPhylogenetic Trees\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nApr 15\n🌱 Trees as Data Objects\n\n\n\n\n\nApr 17\n🌴 Reading and writing trees, ape, ggtree, treeio\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\nApr 22\n🌳 Reshaping Trees\n\n\n\n\n\n\n\n\n\n\n\n\nApr 24\n🌺 Annotating Tree Plots\n📝 Project 3 peer review due\n\n\n\n\n\n\n\n\n\n\nModule 8\n\nOther Topics\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\nApr 29\n🐍 Parsing Data from any Format\n\n\n\n\n\n\n\n\n\n\n\n\nMay 1\n🏆 Final Presentations\n💐 Project 3 presentation (in class)\n\n\n\n\n\n\n\n\n\n\nWeek 16\nMay 6\n🏆 Final Presentations\n💐 Project 3 presentation (in class)\n\n\n\n\n\n\n\n\n\n\n\nMay 7\n\n🎊 Project 3 due"
  },
  {
    "objectID": "schedule.html#schedule-and-course-materials",
    "href": "schedule.html#schedule-and-course-materials",
    "title": "Schedule",
    "section": "",
    "text": "For Qmd files (markdown document with Quarto which are cross-language executable code), go to the course GitHub repository and navigate the directories, or best of all clone the repo and navigate within RStudio or your text browser and R console.\nCurrent Website: https://mbutler808.github.io/rclass-2025/\n2023 Website: https://mbutler808.github.io/rclass/\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopics\nProjects\n\n\n\n\nModule 1\n\nComputational Tools for Data Science\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\nJan 14\n👋 Installing R/RStudio, GitHub [html] [Qmd]\n\nWatch [rest of podcast starting at 4:00] R manual [2.1, 2.2, 2.3, 2.6]\n\n\n\nJan 16\n👩‍💻 Shell scripts, File Organization [html] [Qmd]\n\n\n\n\nWeek 2\nJan 21\n🐙 Intro to Git and GitHub [html] [Qmd]\n🌴 Project 0 [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\nModule 2\n\nReproducible Research\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23\n🐙 Reproducible Research [html] [Qmd]\n\n\n\n\nWeek 3\nJan 28\n🐙 Literate Programming / Intro to Quarto [html] [Qmd]\n\n\n\n\n\nJan 30\n👓 Reference Management [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Analysis in R \n\n\n\n\n\n\n\n\n\n\n\n\nFeb 4\n👓 Reading/Writing data [html] [Qmd]\n🍂 Project 0 due\n\n\n\n\nFeb 6\n👓 Reading/Writing data continued [html] [Qmd]\n\n\n\n\nWeek 4\nFeb 11\n🐙 Scripts [html] [Qmd]\n\n\n\n\n\nFeb 13\n🔩 Data [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Visulization in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\nFeb 18\n🔪 Tidying Data [html] [Qmd]\n🌱 Project 1 assigned\n\n\n\n\nFeb 20\n📊 Plotting Systems (Base R) [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\nFeb 25\n📊 Plotting with ggplot2 [html] [Qmd]\n\n\n\n\n\nFeb 27\n📊 Tidying data with dplyr [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 4\n\nTour of Stats in R and Data Wrangling\n\n\n\n\nWeek 7\nMar 3\n\n🍂 Project 1 due\n\n\n\n\nMar 4\n🐒 Tidyverse and data wrangling [html] [Qmd]\n\n\n\n\n\nMar 6\n🐎 A tour of some univariate methods in R [html] [Qmd]\n🌱 Project 2 assigned\n\n\n\n\n\n\n\n\n\n\nWeek 8\nMar 11\n📆 A tour of some multivariate methods in R [html] [Qmd]\n\n\n\n\n\nMar 11\n🐎 Joining a.k.a. merging data [[html]] [[Qmd]]\n\n\n\n\n\nMar 13\n🐎 Reshaping data [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\nModule 5\n\nProgramming Elements\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\nMar 18\n🤝 Spring Break\n\n\n\n\n\nMar 20\n☀️ Spring Break\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\nMar 20\n🍄 Functions [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25\n🌵 Lists and For Loops [[html]] [[Qmd]]\n🍂 Project 2 due\n\n\n\n\n\n\n\n\n\n\n\nMar 27\n🌵 Apply Functions [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nApr 1\n🎡 Program Flow [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\nModule 6\n\nIntro to Morphometrics\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3\n📏 Intro to Morphometrics [[html]] [[Qmd]]\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nApr 8\n📐 Landmark-based Morphometrics\n\n\n\n\n\n\n\n\n\n\n\n\nApr 10\n💀 3D Morphometrics\n🌰 Project 3 data check due\n\n\n\n\n\n\n\n\n\n\nModule 7\n\nPhylogenetic Trees\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nApr 15\n🌱 Trees as Data Objects\n\n\n\n\n\nApr 17\n🌴 Reading and writing trees, ape, ggtree, treeio\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\nApr 22\n🌳 Reshaping Trees\n\n\n\n\n\n\n\n\n\n\n\n\nApr 24\n🌺 Annotating Tree Plots\n📝 Project 3 peer review due\n\n\n\n\n\n\n\n\n\n\nModule 8\n\nOther Topics\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\nApr 29\n🐍 Parsing Data from any Format\n\n\n\n\n\n\n\n\n\n\n\n\nMay 1\n🏆 Final Presentations\n💐 Project 3 presentation (in class)\n\n\n\n\n\n\n\n\n\n\nWeek 16\nMay 6\n🏆 Final Presentations\n💐 Project 3 presentation (in class)\n\n\n\n\n\n\n\n\n\n\n\nMay 7\n\n🎊 Project 3 due"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "",
    "text": "Delivery: In person\nCourse time: Tuesdays and Thursdays from 9-10:15am\nCourse location: KELLER 204\nAssignments: Weekly small quizzes, four projects\n\n\n\nTo add the course: Let me know so I can give you an override\nRegister for ZOOL710 CRN 89354 3 credits.\nAttendance is highly recommended for personal help, Q&A, and group work. Repetition is key.\nPlease contact course instructor if interested in auditing.\nUndergraduates are welcome to join with approval.\n\n\nMarguerite A. Butler (https://butlerlab.org)\n\nOffice Location: Edmondson 318\nEmail: mbutler808 at gmail.com\nOffice Hours: After class and by appointment\n\n\n\nIn order of preference, here is a preferred list of ways to get help:\n\nI strongly encourage you to use the course DISCORD server, before joining office hours. You can get your answers faster, and other students in the class (who likely have similar questions) can also benefit from the questions and answers given. Everyone is encouraged to participate.\nSee me after class.\nMake an appointment by email."
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "",
    "text": "Delivery: In person\nCourse time: Tuesdays and Thursdays from 9-10:15am\nCourse location: KELLER 204\nAssignments: Weekly small quizzes, four projects\n\n\n\nTo add the course: Let me know so I can give you an override\nRegister for ZOOL710 CRN 89354 3 credits.\nAttendance is highly recommended for personal help, Q&A, and group work. Repetition is key.\nPlease contact course instructor if interested in auditing.\nUndergraduates are welcome to join with approval.\n\n\nMarguerite A. Butler (https://butlerlab.org)\n\nOffice Location: Edmondson 318\nEmail: mbutler808 at gmail.com\nOffice Hours: After class and by appointment\n\n\n\nIn order of preference, here is a preferred list of ways to get help:\n\nI strongly encourage you to use the course DISCORD server, before joining office hours. You can get your answers faster, and other students in the class (who likely have similar questions) can also benefit from the questions and answers given. Everyone is encouraged to participate.\nSee me after class.\nMake an appointment by email."
  },
  {
    "objectID": "syllabus.html#important-links",
    "href": "syllabus.html#important-links",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Important Links",
    "text": "Important Links\n\nCourse website: coming soon.\nGitHub repository with all course material: coming soon.\nDiscord server: https://discord.gg/fagxUbq5Rd"
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\nUpon successfully completing this course, students will be able to:\n\nInstall and configure software necessary for a statistical programming environment\nDiscuss generic programming language concepts as they are implemented in a high-level statistical language\nWrite, debug, and comment your code in base R and the tidyverse\nBuild basic data visualizations using R and the tidyverse\nDiscuss best practices for coding and reproducible research, basics of data ethics and management, basics of working with special data types, and basics of storing data\nDocument and communicate your findings via reports produced in Quarto/Rmarkdown\nArchive and share your data analysis pipeline and reports via GitHub, and understand the basics of a collaborative coding project"
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Lectures",
    "text": "Lectures\nLectures will be in person in Keller 204 from 9-10:15 am on Tuesdays and Thursdays."
  },
  {
    "objectID": "syllabus.html#textbook-and-other-course-material",
    "href": "syllabus.html#textbook-and-other-course-material",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Textbook and Other Course Material",
    "text": "Textbook and Other Course Material\nThere is no required textbook. We will make use of several freely available textbooks and other materials. All course materials will be provided. We will use the R software for data analysis, and git for version control and data sharing, all of which is freely available for download."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Software",
    "text": "Software\nPlease install R onto your laptop. You can obtain R from the Comprehensive R Archive Network. There are versions available for Mac, Windows, and Unix/Linux. This software is required for this course.\nIt is important that you have the latest version of R installed. For this course we will be using R version 4.4.2 or higher. You can determine what version of R you have by starting up R and typing into the console R.version.string and hitting the return/enter key. If you do not have the proper version of R installed, go to CRAN and download and install the latest version.\nSome students like to use the Rstudio interface, but this is optional, and in fact discouraged until you have a grasp of the R environment (I will let you know when we are at a good place in the course). The RStudio interactive development environment (IDE) requires that R be installed, and so is an “add-on” to R. You can obtain the RStudio Desktop for free from the RStudio web site. You can determine the version of RStudio by looking at menu item Help &gt; About RStudio. You should be using RStudio version 1.4.1106 or higher."
  },
  {
    "objectID": "syllabus.html#quizzes",
    "href": "syllabus.html#quizzes",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Quizzes",
    "text": "Quizzes\nThere will be weekly (short) quizzes on Laulima in the beginning of the semester. These are intended to be low-stakes to assist you in checking your understanding of R syntax and get you more comfortable with trial-and-error learning."
  },
  {
    "objectID": "syllabus.html#projects",
    "href": "syllabus.html#projects",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Projects",
    "text": "Projects\nThere will be one optional assignment and 4 graded assignments, due every 3–4 weeks. Projects will be submitted electronically via GitHub (more on this later).\nThe projects are basically a scaffold to learn how to build a data analysis pipeline for your own research data. If you donʻt have your own data yet, I encourage students to ask their advisor for a sample dataset, or a published dataset, or another grad studentʻs dataset to practice on. You can also ask me for help to find data, this is not a problem.\nProject 0 is actually optional, but you are encouraged to practice by putting up your own website. Project 1 is data cleaning on a sample dataset, Project 2 produces analyses on the cleaned data from Project 1. Project 3 is applying everything you learned to your own dataset and exploring. You will also do a show-and-tell oral presentation on Project 3 at the end of the semester. Itʻs fun and students learn a lot. Itʻs exciting to see everyone elseʻs stuff and the diversity of projects people do.\nThe project assignments will be due on\n\nProject 0: February 4, 11:59pm (optional and not graded but hopefully useful and fun)\nProject 1: February 27, 11:59pm\nProject 2: March 25, 11:59pm\nProject 3: April 7-May 6 (multiple stages)\n\nCollaboration\nPlease feel free to study together and talk to one another about project assignments. The mutual instruction that students give each other is among the most valuable that can be achieved.\nHowever, it is expected that project assignments will be implemented and written up independently unless otherwise specified. Specifically, please do not share analytic code or output. Please do not collaborate on write-up and interpretation. Please do not access or use solutions from any source before your project assignment is submitted for grading."
  },
  {
    "objectID": "syllabus.html#discussion-forum",
    "href": "syllabus.html#discussion-forum",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Discussion Forum",
    "text": "Discussion Forum\nThe course will make use of DISCORD to ask and answer questions and discuss any of the course materials. Please engage and provide answers as well as questions. The Instructor will monitor DISCORD and answer questions when appropriate."
  },
  {
    "objectID": "syllabus.html#exams",
    "href": "syllabus.html#exams",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Exams",
    "text": "Exams\nThere are no exams in this course."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Grading",
    "text": "Grading\nGrades in the course will be based on weekly quizzes (10%), participation (20%) and projects (70%). Each of Projects 1–3 counts approximately equally in the final grade. Grades will be posted on Laulima."
  },
  {
    "objectID": "syllabus.html#policy-for-submitted-projects-late",
    "href": "syllabus.html#policy-for-submitted-projects-late",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Policy for submitted projects late",
    "text": "Policy for submitted projects late\nThe policy for late submissions is as follows:\n\nEach student will be given two free “late days” for the rest of the course.\nA late day extends the individual project deadline by 24 hours without penalty.\nThe late days can be applied to just one project (e.g. two late days for Project 2), or they can be split across the two projects (one late day for Project 2 and one late day for Project 3). This is entirely left up to the discretion of the student.\nLate days are intended to give you flexibility: you can use them for any reason no questions asked.\nYou do not get any bonus points for not using your late days, and they are not transferrable.\n\nFor students who exceed their free late days:\n\nI will be deducting 5% for each extra late day. For example, if you have already used all of your late days for the term, we will deduct 5% for the assignment that is &lt;24 hours late, 10% points for the assignment that is 24-48 hours late, and 15% points for the assignment that is 48-72 hours late, etc.\n\nI will not grade assignments that are more than 3 days past the original due date.\n\nRegrading Policy\nIt is very important to me that all assignments are properly graded. If you believe there is an error in your assignment grading, please send an email within 7 days of receiving the grade explaining the issue. No re-grade requests will be accepted orally, and no regrade requests will be accepted more than 7 days after you receive the grade for the assignment."
  },
  {
    "objectID": "syllabus.html#academic-ethics-and-student-conduct-code",
    "href": "syllabus.html#academic-ethics-and-student-conduct-code",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Academic Ethics and Student Conduct Code",
    "text": "Academic Ethics and Student Conduct Code\nThe faculty, staff, and students participating in courses of the School of Life Sciences assume a responsibility to uphold the Universityʻs missions of academic excellence and social responsibility as appropriate for an institute of higher education. Violations of the UH Systemwide Student Conduct Code includes but is not limited to: cheating; plagiarism; providing copies of your work to other students which is submitted as their own; obtaining copies of said work by others; using copies of said work or representing any portion of another person’s work as your own (i.e., plagiarism); misconduct. While we encourage you to discuss strategies for problem solving, and even collaborate by working through the problems/strategies together, giving someone all the answers is cheating. If you are unsure please ask.\nPlagiarism is when you use information or present ideas, whether by paraphrase or direct quote, from a source (be it published or a classmate) without giving proper credit to that source. Cheating in any way will be reported to the attention of UH Office of Judicial Affairs, and result in an F in this course. Students should be familiar with the policies and procedures specified under the Systemwide Student Conduct Code portal."
  },
  {
    "objectID": "syllabus.html#disability-support-service",
    "href": "syllabus.html#disability-support-service",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Disability Support Service",
    "text": "Disability Support Service\nStudents requiring accommodations for disabilities should register with the Kokua program at Student Disability Services. It is the responsibility of the student to register for accommodations. The Kokua office will send me a notification once you are registered, however, they often do not share information regarding the specifics. If the accommodations are not sufficient to ensure your success, please contact me as soon as possible so that we may work together on providing for an effective learning environment."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis is an applied quantitative course. I will not discuss the mathematical details of specific data analysis approaches, however some statistical background and being comfortable with quantitative thinking is useful. Previous experience with writing computer programs in general and R in particular is also helpful, but not necessary. If you have no programming experience, expect to spend extra time getting yourself familiar with R, especially at the very beginning (but it will get better). As long as you are willing to invest the time to learn the programming and you do not mind thinking quantitatively, you should be able to do well, independent of your background. In fact you will have the most to gain.\nGetting set up\nYou must install R and RStudio, optional on your computer in order to complete this course. These are two different applications that must be installed separately before they can be used together:\n\nR is the core underlying programming language and computing engine that we will be learning in this course\nRStudio is an interface into R that makes many aspects of using and programming R simpler\n\nBoth R and RStudio are available for Windows, macOS, and most flavors of Unix and Linux. Please download the version that is suitable for your computing setup.\nThroughout the course, we will make use of numerous R add-on packages that must be installed over the Internet. Packages can be installed using the install.packages() function in R. For example, to install the tidyverse package, you can run\n\ninstall.packages(\"tidyverse\")\n\nin the R console.\nHow to Download R for Windows\nGo to https://cran.r-project.org and\n\nClick the link to “Download R for Windows”\nClick on “base”\nClick on “Download R 4.2.2 for Windows”\n\n\n\n\n\n\n\nWarning\n\n\n\nFor all software, please download the latest version.\n\n\nHow to Download R for the Mac\nGoto https://cran.r-project.org and\n\nClick the link to “Download R for (Mac) OS X”.\nClick on “R-4.2.2.pkg” (or the latest version)\nHow to Download RStudio\nGoto https://rstudio.com and\n\nClick on “Products” in the top menu\nThen click on “RStudio” in the drop down menu\nClick on “RStudio Desktop”\nClick the button that says “DOWNLOAD RSTUDIO DESKTOP”\nClick the button under “RStudio Desktop” Free\nUnder the section “All Installers” choose the file that is appropriate for your operating system."
  },
  {
    "objectID": "syllabus.html#general-disclaimers",
    "href": "syllabus.html#general-disclaimers",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "General Disclaimers",
    "text": "General Disclaimers\n\nThis syllabus is a general plan, deviations announced to the class by the instructor may be necessary."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Projects 2,3\n\n\n\n\n\n\nproject 2\n\n\nproject 3\n\n\nprojects\n\n\n\nInformation for Projects 2 and 3\n\n\n\n\n\nFeb 11, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nProject 0 (optional)\n\n\n\n\n\n\nproject 0\n\n\nprojects\n\n\n\nInformation for Project 0 (entirely optional, but hopefully useful and fun!)\n\n\n\n\n\nJan 14, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nOral Presentation\n\n\n\n\n\n\nproject 3\n\n\nprojects\n\n\n\nInformation for Final Oral Presenatation\n\n\n\n\n\nApr 27, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nProject 1\n\n\n\n\n\n\nproject 1\n\n\nprojects\n\n\n\nInformation for Project 1\n\n\n\n\n\nJan 30, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2025-01-14-project-0/index.html",
    "href": "projects/2025-01-14-project-0/index.html",
    "title": "Project 0 (optional)",
    "section": "",
    "text": "This exercise is modified from material developed by Stephanie Hicks."
  },
  {
    "objectID": "projects/2025-01-14-project-0/index.html#acknowledgements",
    "href": "projects/2025-01-14-project-0/index.html#acknowledgements",
    "title": "Project 0 (optional)",
    "section": "",
    "text": "This exercise is modified from material developed by Stephanie Hicks."
  },
  {
    "objectID": "projects/2025-01-28-project-1/index.html",
    "href": "projects/2025-01-28-project-1/index.html",
    "title": "Project 1",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "projects/2025-01-28-project-1/index.html#acknowledgements",
    "href": "projects/2025-01-28-project-1/index.html#acknowledgements",
    "title": "Project 1",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "posts/2025-02-11-scripts/index.html",
    "href": "posts/2025-02-11-scripts/index.html",
    "title": "Saving your work as R scripts",
    "section": "",
    "text": "Watch ahead\n\n\n\nBefore class, you can prepare by watching this podcast:\n\n\n\n\n✏️"
  },
  {
    "objectID": "posts/2025-02-11-scripts/index.html#make-sure-you-can-see-file-endings",
    "href": "posts/2025-02-11-scripts/index.html#make-sure-you-can-see-file-endings",
    "title": "Saving your work as R scripts",
    "section": "Make sure you can see file endings",
    "text": "Make sure you can see file endings\nDoes your MacOS or Windows environment show you the file endings (i.e., .R, .pdf, .csv, etc.)? If not be sure to turn them on. Try the instructions below or you can google for “show all file exensions in” (Mac or Windows, etc.).\nMac OS\nThis is a Finder preference. From any Finder window, click on the menu bar: Finder &gt;  Preferences &gt; Advanced &gt; Click on Show all finename extensions.\nWindows\nThis is in File Explorer (Windows key + E). Click on the menu: View &gt; Show &gt; File Name Extensions. You can also choose to show all hidden files if you wish. For pictures see here"
  },
  {
    "objectID": "posts/2025-02-11-scripts/index.html#text-editor-environment",
    "href": "posts/2025-02-11-scripts/index.html#text-editor-environment",
    "title": "Saving your work as R scripts",
    "section": "Text editor environment",
    "text": "Text editor environment\nWhile I love the R text editor for writing R scripts, for working with multiple .qmd and other files I find it helpful to have a full-featured plain text editor. A new tool that I discovered is the Sublime text editor. If youʻd like to try it out, you can download it here: https://www.sublimetext.com\nA couple of features I like is that you can have multiple panes open. For example if you want to copy text from an old script to a new script, you can easily see and do that.\nIt also allows you to organize Projects, various folders that will appear on the sidebar to preserve your workspace. This helps when you are writing text documents across folders. So for example if you have your Rclass folder in one place and your website folder in another, you can have both open within the Sublime project. When you finish working on it you can save the project and reopen it later.\nTo create a project start by opening a new file. Then choose the Project &gt; Add folder to Project... on the menu bar. You can load mulitple folders.\nIt has contextual highlighting for Quarto as well as GitHub markdown.\nIt also has integration with command line R https://bishwarup-paul.medium.com/a-guide-to-using-r-in-sublime-text-27f78b33f872. You can run R commands in a lower terminal pane, sent directly from your text document in sublime."
  },
  {
    "objectID": "posts/2025-02-11-scripts/index.html#different-desktop-windows",
    "href": "posts/2025-02-11-scripts/index.html#different-desktop-windows",
    "title": "Saving your work as R scripts",
    "section": "Different Desktop Windows",
    "text": "Different Desktop Windows\nItʻs also nice to have multiple desktops to organize your work. - It makes it easier to find your different apps. - You may have one workspace for your text editor, and another for your Terminal or CMD prompt, for example. - If I am working with multiple git repos, I might have one desktop just for my Terminal windows with a separate Terminal open for each one.\nTo use multiple desktops: On Windows On Mac\nOne the Mac, you can open new desktops by using three fingers to swipe up on the trackpad. Switch between them by swiping left or right with three fingers."
  },
  {
    "objectID": "posts/2025-02-11-scripts/index.html#organize-your-projects-into-folders",
    "href": "posts/2025-02-11-scripts/index.html#organize-your-projects-into-folders",
    "title": "Saving your work as R scripts",
    "section": "Organize your Projects into Folders",
    "text": "Organize your Projects into Folders\nWeʻve been learning about reproducibility. One important aspect is file organization. Each project should be organized into one folder that contains:\n\nAll input data (usually in a Data folder)\nAll code and documentation\nAll output\n\nThe idea is to keep everything complete, self-contained, and clear. Move old versions into a “Trash” folder. If you donʻt end up looking back at it, then delete it! (Or if you are bold, delete it right away!)\nA really useful UNIX/CMD command is tree. It shows you the directory structure contained within any folder. It works on both MacOS and Windows.\nThis is in ASCII – so you can copy and paste it into your README.md file!\n\n\nTerminal/CMD\n\ntree myfolder\n\nIf it is not pre-installed on your mac, you may need to install it with homebrew:\n\n\nTerminal\n\nbrew install tree"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html",
    "href": "posts/2025-01-28-intro-quarto/index.html",
    "title": "Literate Statistical Programming and Quarto",
    "section": "",
    "text": "Before class, you can prepare by installing the following materials:\n\n\n\n\nInstall quarto https://quarto.org/docs/get-started/\n\nInstall tinytex, a version of \\(\\LaTeX\\) to render output. Open a Terminal window (git-BASH on Windows) and type:\n\n\n\nTerminal\n\nquarto install tinytex\n\nNote: This will not add tinytex to the system PATH so if you would like to use tinytex with other applications, or you have a different installation of TeX that you prefer to use instead with quarto, see notes here https://quarto.org/docs/output-formats/pdf-engine.html.\n\nInstall pandoc, which translates output to almost any format. https://pandoc.org/installing.html Follow instructions for whichever installer you prefer. If you are using homebrew on a mac:\n\n\n\nTerminal\n\nbrew install pandoc\n\n\nInstall rmarkdown (“R Markdown), which quarto builds upon. rmarkdown also includes the package knitr which executes the R code and provides \\(\\LaTeX\\) formatting for the output. From within your R console:\n\n\n\nR_Console\n\ninstall.packages(\"rmarkdown\")\n\n\n\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nCreating a Website in Quarto quickstart up to and including Render https://quarto.org/docs/websites/\n\nPublishing to GitHub up to and including Render to docs https://quarto.org/docs/publishing/github-pages.html\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-01-literate-programming/\nhttp://users.stat.umn.edu/~geyer/Sweave/\nhttps://rdpeng.github.io/Biostat776/lecture-literate-statistical-programming.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#install",
    "href": "posts/2025-01-28-intro-quarto/index.html#install",
    "title": "Literate Statistical Programming and Quarto",
    "section": "",
    "text": "Before class, you can prepare by installing the following materials:\n\n\n\n\nInstall quarto https://quarto.org/docs/get-started/\n\nInstall tinytex, a version of \\(\\LaTeX\\) to render output. Open a Terminal window (git-BASH on Windows) and type:\n\n\n\nTerminal\n\nquarto install tinytex\n\nNote: This will not add tinytex to the system PATH so if you would like to use tinytex with other applications, or you have a different installation of TeX that you prefer to use instead with quarto, see notes here https://quarto.org/docs/output-formats/pdf-engine.html.\n\nInstall pandoc, which translates output to almost any format. https://pandoc.org/installing.html Follow instructions for whichever installer you prefer. If you are using homebrew on a mac:\n\n\n\nTerminal\n\nbrew install pandoc\n\n\nInstall rmarkdown (“R Markdown), which quarto builds upon. rmarkdown also includes the package knitr which executes the R code and provides \\(\\LaTeX\\) formatting for the output. From within your R console:\n\n\n\nR_Console\n\ninstall.packages(\"rmarkdown\")\n\n\n\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nCreating a Website in Quarto quickstart up to and including Render https://quarto.org/docs/websites/\n\nPublishing to GitHub up to and including Render to docs https://quarto.org/docs/publishing/github-pages.html"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#acknowledgements",
    "href": "posts/2025-01-28-intro-quarto/index.html#acknowledgements",
    "title": "Literate Statistical Programming and Quarto",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-01-literate-programming/\nhttp://users.stat.umn.edu/~geyer/Sweave/\nhttps://rdpeng.github.io/Biostat776/lecture-literate-statistical-programming.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#weaving-and-tangling",
    "href": "posts/2025-01-28-intro-quarto/index.html#weaving-and-tangling",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Weaving and Tangling",
    "text": "Weaving and Tangling\nLiterate programs by themselves are a bit difficult to work with, but they can be processed in two important ways.\nLiterate programs can be weaved to produce human readable documents like PDFs or HTML web pages, and they can tangled to produce machine readable code. To do this we need a documentation language."
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#sweave-and",
    "href": "posts/2025-01-28-intro-quarto/index.html#sweave-and",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Sweave and \\(\\LaTeX\\)\n",
    "text": "Sweave and \\(\\LaTeX\\)\n\nThe first literate programming system for R is called Sweave. sweave enables users to combine R code with a typesetting language called \\(\\LaTeX\\).\n\\(\\LaTeX\\) markup specifies how the text, mathematical equations, tables, and figures will be formatted, allowing the separation of content from formatting (as opposed to WYSYWIG word processors like Word or Google Docs).\n\\(\\LaTeX\\) is very powerful for:\n\nLaying out mathematical equations. This is its superpower!\n\nFine-tuned control of formatting (all kinds!)\nPowerful tools for integrating citations via BibTEX and cross-references to Figures, Tables, Equations, Sections, anything you wish to label.\n\n\\(\\LaTeX\\) was written by Leslie Lamport when he was at the Stanford Research Institute, as a set of macros for \\(\\TeX\\), which is the underlying typesetting engine written by Donald Knuth. Lamport wanted to write a book using \\(\\TeX\\), so he wrote the \\(\\LaTeX\\) macros and a manual, which made it easier for many others to use \\(\\TeX\\).\nBefore sweave, one would write code to perform statistical analysis or modeling in R, and the manuscript in latex once the coding was complete. As projects develop, however, keeping these processes separate can lead to a lack of reproducibility.\nsweave revolutionized coding, and has become part of the R base code. sweave was written by Friedrich Leisch, who is on the R Core Development Team and the BioConductor Project. To see an example of LaTeX see [small2e.tex] and for Sweave, check out [Little_Sweave.Rnw].\nSweave files have a .Rnw file ending and have R code weaved through the document, interspered with :\n&lt;&lt;plot1, height=4, width=5, eval=FALSE&gt;&gt;=\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n@\nOnce you have created your .Rnw file containing code chunks for your R code, and \\(\\LaTeX\\) tags to format your text, from your R console you run the sweave() function to weave the file, executing the R chunks and replacing them with output as appropriate to typeset before creating the PDF document.\nSweaveʻs main limitation is that it requires knowledge of \\(\\LaTeX\\), which has a steep learning curve.\n\nSweave also lacks a lot of features that people find useful like caching, and multiple plots per page and mixing programming languages.\nFor developers, Sweave is hard to extend with new functions.\n\nInspired by Sweave, Yihui Xie wrote knitr to extend literate programming capabilities in R even further by combining features of other add on packages into one package. It adds richer graphics features and importantly extends use of much simpler Markdown documents to take advantage of LaTeX without having to write it!"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#rmarkdown",
    "href": "posts/2025-01-28-intro-quarto/index.html#rmarkdown",
    "title": "Literate Statistical Programming and Quarto",
    "section": "rmarkdown",
    "text": "rmarkdown\nMarkdown is a much simpler document language. A markdown file is a plain text file that is typically given the extension .md. The rmarkdown R package takes a R Markdown file (.Rmd) and weaves together R code chunks Figure 1, which can be output in a variety of different possible formats.\n\n\n\n\n\nFigure 1: R markdown can be used to translate text and code into many different output formats\n\n\nR chunks surrounded by text looks like this:\n```{r plot1, height=4, width=5, eval=FALSE, echo=TRUE}\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n```\n\n\n\n\n\n\nTip\n\n\n\nThe best resource for learning about R Markdown this the book by Yihui Xie, J. J. Allaire, and Garrett Grolemund:\n\nhttps://bookdown.org/yihui/rmarkdown\n\nThe R Markdown Cookbook by Yihui Xie, Christophe Dervieux, and Emily Riederer is really good too:\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook\n\nThe authors describe the motivation for the 2nd book as:\n\n“However, we have received comments from our readers and publisher that it would be beneficial to provide more practical and relatively short examples to show the interesting and useful usage of R Markdown, because it can be daunting to find out how to achieve a certain task from the aforementioned reference book (put another way, that book is too dry to read). As a result, this cookbook was born.”\n\n\n\nBecause this is lecture is built using a .qmd file (which is very similar to a .Rmd file), let’s demonstrate how this works. I am going to change eval=FALSE to eval=TRUE.\n\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy do we not see the back ticks ``` anymore in the code chunk above that made the plot?\nWhat do you think we should do if we want to have the code executed, but we want to hide the code that made it?\n\n\n\nBefore we leave this section, I find that there is quite a bit of terminology to understand the magic behind rmarkdown that can be confusing, so let’s break it down:\n\n\nPandoc. Pandoc is a command line tool with no GUI that converts documents (e.g. from number of different markup formats to many other formats, such as .doc, .pdf etc). It is completely independent from R (but does come bundled with RStudio). If you donʻt have Rstudio installed, you will have to install pandoc.\n\nMarkdown (markup language). Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension .md. It is completely independent from R.\nR Markdown (markup language). R Markdown is an extension of the markdown syntax for weaving together text with R code. R Markdown files are plain text files that typically have the file extension .Rmd.\n\nrmarkdown (R package). The R package rmarkdown is a library that uses pandoc to process and convert text and R code written in .Rmd files into a number of different formats. This core function is rmarkdown::render(). Note: this package only deals with the markdown language. If the input file is e.g. .Rhtml or .Rnw, then you need to use knitr prior to calling pandoc (see below).\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the R Markdown Quick Tour for more:\n\nhttps://rmarkdown.rstudio.com/authoring_quick_tour.html\n\n\n\n\n\nArtwork by Allison Horst on RMarkdown\n\nknitr\nOne of the alternative that has come up in recent times is something called knitr.\n\nThe knitr package for R takes a lot of these ideas of literate programming and updates and improves upon them.\n\nknitr still uses R as its programming language, but it allows you to mix other programming languages in.\n\nknitr is included in the installation of rmarkdown.\nYou can also use a variety of documentation languages now, such as \\(\\LaTeX\\) , markdown and HTML.\n\nknitr was developed by Yihui Xie while he was a graduate student at Iowa State and it has become a very popular package for writing literate statistical programs.\n\nKnitr takes a plain text document with embedded code, executes the code and ‘knits’ the results back into the document.\nFor for example, it converts\n\nAn R Markdown (.Rmd) file into a standard markdown file (.md)\nAn .Rnw (Sweave) file into to .tex format.\nAn .Rhtml file into to .html.\n\nThe core function is knitr::knit() and by default this will look at the input document and try and guess what type it is e.g. Rnw, Rmd etc.\nThis core function performs three roles:\n\nA source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.\nA code evaluator, which evaluates this code\nAn output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type. For instance, if the input file is an .Rmd, the output render marks up the output of code evaluation in .md format.\n\n\n\n\n\nConverting a Rmd file to many outputs using knitr and pandoc\n\n\n\n[Source]\nAs seen in the figure above, from there pandoc is used to convert e.g. a .md file into many other types of file formats into a .html, etc.\nSo in summary:\n\n“R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on).”\n\n[Source]"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#create-your-website-locally-with-quarto",
    "href": "posts/2025-01-28-intro-quarto/index.html#create-your-website-locally-with-quarto",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Create your website locally with Quarto",
    "text": "Create your website locally with Quarto\nIn this section, I am adding a bit more explanation to the Quarto quickstart guide up to and including Render. If something is not clear, please consult https://quarto.org/docs/websites/\nThere are three main quarto commands we will use:\n\n\nquarto create-project: Make a website project template\n\nquarto preview: Take a look at what the webite will look like\n\nquarto render: Render your qmd to html\n\n\nMake your website directory and template\nCreate your website (here called mysite) using the following command. It will make a directory of the same name and put the website contents within it.\n\n\nTerminal\n\nquarto create-project mysite --type website\n\nYou should now see the following files in your mysite directory (Figure 2):\n\n\n\n\n\nFigure 2: Website files from the Terminal view\n\n\nThis is the bare-bones version of your website. Check that the code is functional by looking at a preview:\n\n\nTerminal\n\nquarto preview\n\nThis should open up a browser window showing a temporary file made by quarto by rendering your website files.\n\n\n\n\n\n\nTip\n\n\n\n\n\nquarto preview will refresh the preview every time you save your index.qmd (or any) website files. So itʻs a good idea to keep the preview open as you make edits and saves.\nCheck every edit, it is easier to debug in small steps.\nTerminate quarto preview with Control-c\n\n\n\n\nRender your website to html\nUse quarto to render your content to html, the format used by browsers. First navigate into your website directory then render:\n\n\nTerminal\n\ncd mysite\nquarto render\n\nTake a look at the mysite contents after rendering, you should see a new directory _site (Figure 3). The html was rendered and put in there (go ahead, open up the files and check it out):\n\n\n\n\n\nFigure 3: Website files after rendering\n\n\nPersonalize your content\nWhat is really nice is that you can personalize your website by simply editing the quarto markdown and yaml files.\nWeb content goes in .qmd\n\nUsing any text editor, edit the index.qmd to personalize your website.\nThe first section of your index.qmd is the header. You can change the title and add additional header information, including any cover images and website templates.\nFor example this is what I have in my course website index.qmd header. Note that my cover image is in a folder called images within at the top level of my website directory. If you want to try this out substitute or remove the image line and change the twitter/github handles.\n\n\nindex.qmd\n\n---\ntitle: \"Welcome to Introduction to Data Science in R for Biologists!\"\nimage: images/mycoolimage.png\nabout:\n  template: jolla\n  links:\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com/mbutler808\n    - icon: github\n      text: Github\n      href: https://github.com/mbutler808\n---\n\nYou should edit the body of your website as well. You simply edit the text.\nThe quarto markdown page has great examples showing how to format your content. Take a look at how to specify header sizes, lists, figures and tables. You might want to bookmark this page!\nTry editing the about.qmd file as well. You will notice that this is another tab in your website. YOu can add more tabs by adding .qmd files.\nWith each addition, be sure to quarto preview your changes to make sure it works. When you are satisfied with your website, quarto render to render to html.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen editing markdown, take care to note spaces and indents as they are interpreted for formatting.\nIndentations are really important for formatting lists.\nFor example in a hyperlink, there is no space between the square brackets and parentheses. [This is a cool link](http://mycoollink.com)\n\n\n\n\nWebsite-wide settings go in _quarto.yml\n\nAll Quarto projects include a _quarto.yml configuration file that sets the global options that apply across the entire website.\nYAML started off as “Yet Another Markup Language” 😜. It is clean, clear, and widely used. You can edit your YAML to add options or change the format of your website. Take a look at your _quarto.yml.\nHere is an example for a simple website. title: is the parameter to set the websiteʻs title. navbar: sets the menu, in this case on the left sidebar. By default tabs will be named based on the names of the .qmd files, but you can set them manually. There are many themes you can choose from too, check them out. For something different try cyborg.\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  title: \"today\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: minty\n    css: styles.css\n    toc: true\n\nAgain, after saving your edits, quarto preview to see the effects. When you are satisfied with your website, quarto render to render to html.\n\n\nTerminal\n\nquarto render"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#publishing-your-website-to-github",
    "href": "posts/2025-01-28-intro-quarto/index.html#publishing-your-website-to-github",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Publishing your website to GitHub",
    "text": "Publishing your website to GitHub\nYou can publish your website for free on GitHub, which is a very cool feature. In his section I am adding a bit more explanation to the Quarto quickstart guide up to and including Render to docs https://quarto.org/docs/publishing/github-pages.html. I describe the most important stpes below:\n\nRender your html to a docs directory\nSupress GitHub jekyll html processing by creating a .nojekyll file\nMake your website directory into a repo, and link it to a GitHub repo\nEdit the GitHub repo settings to publish your website\n\nRender your html to docs\n\nEdit the _quarto.yml file at the top level of your website to send output to docs. This will also create the docs folder.\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n\nThe next time you quarto render it will create docs and all of its contents.\nSupress GitHub jekyll html processing\nGitHub uses a sofware called jekyll to render html from markdown. Since weʻre using quarto, we want to supress that. Create an empty file named .nojekyll at the top level of your website directory to supress default jekyll processing.\n\n\n\n\n\n\n\nMac/Linux/ Git-bash\n\n\nTerminal\n\ntouch .nojekyll\n\n\n\nWindows\n\n\nCMD\n\ncopy NUL .nojekyll\n\n\n\nSetup a GitHub repo for your website\n\nTurn your website directory into a git repo:\n\n\n\nTerminal\n\ngit init\ngit add .\ngit commit -m \"first commit\"\n\n\nCreate a GitHub repo by the same name\n\nFor example, mine might be github.com/mbutler808/mysite.\n\nLink your local repo and GitHub repo together\n\nIf you forgot how to do this, go back here\n\nCheck your GitHub repo. Are your files there?\nGitHub settings to serve your webpage\nAlmost there! A couple more steps.\nFrom your GitHub repo, click on Settings in the top menu, and Pages on the left menu.\nYour website should deploy from branch. Under Select branch choose main and under Select folder choose docs.\nAfter clicking save GitHub will trigger a deployment of your website. After a few minutes, your URL will appear near the top at Your site is live at...:\n\nCongratulations! ⚡️ Your website is now live 🎉🎊😍"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#now-make-more-changes",
    "href": "posts/2025-01-28-intro-quarto/index.html#now-make-more-changes",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Now make more changes!",
    "text": "Now make more changes!\n\n\n\n\n\n\nThe Quarto Workflow is\n\n\n\n\nEdit the content in .qmd\nFrom the Command line:\n\n\nquarto preview to check that edits are correct\n\nquarto render to render .qmd to .html\n\ngit add .\ngit commit -m \"message\"\ngit push origin main\n\n\nCheck your website (this may take a beat)"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#for-fun",
    "href": "posts/2025-01-28-intro-quarto/index.html#for-fun",
    "title": "Literate Statistical Programming and Quarto",
    "section": "For fun",
    "text": "For fun\nYou can have fun with emoji! Guangchuang Yu wrote the package emojifont (this is the same person who wrote the widely used ggtree package) and now you can bring your emoji out of your phone and into your quarto documents! Install the R package emojifont:\n\ninstall.packages(\"emojifont\")\n\nThen anywhere you want an emoji in the markdown file, you just type:\n\n`r emojifont::emoji('palm_tree')`\n\n🌴\nOr if you want several, just line them up:\n\n`r emojifont::emoji('balloon')``r emojifont::emoji('tada')``r emojifont::emoji('smiley')`\n\n🎈🎉😃\nThere is a handy cheat sheet of emoji names here https://gist.github.com/rxaviers/7360908"
  },
  {
    "objectID": "posts/2025-01-28-intro-quarto/index.html#final-tips",
    "href": "posts/2025-01-28-intro-quarto/index.html#final-tips",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Final tips",
    "text": "Final tips\n\n\n\n\n\n\nTip\n\n\n\n\n\nAlways always quarto render before you push up your changes to GitHub!\nIf your changes are not appearing, try quarto preview and check that your changes appear in the preview. Then quarto render before you use git to add, commit, and push\nNote: It can take a few minutes to render on GitHub before your changes appear on your website\n\n\n\nPlease see Stephanie Hicksʻ lecture for more literate programming examples and tips."
  },
  {
    "objectID": "posts/2025-03-06-univariate/index.html",
    "href": "posts/2025-03-06-univariate/index.html",
    "title": "A small review of univariate parametric statistics",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to perform basic univariate statistics\nBe able to design plots to display univariate comparisons\nBe able to relate questions to graphical representations of data"
  },
  {
    "objectID": "posts/2025-03-06-univariate/index.html#linear-regression",
    "href": "posts/2025-03-06-univariate/index.html#linear-regression",
    "title": "A small review of univariate parametric statistics",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression asks whether there is a relationship between X and Y, that is if you know X can you predict the value of Y?\n\nwith(iris, plot(Sepal.Width ~ Sepal.Length))\nlm.fit &lt;- with(iris, lm(Sepal.Width ~ Sepal.Length))\nabline(lm.fit, col=\"blue\")\n\n\n\n\n\n\n\nLinear regression results in two parameters, the best-fit slope and intercept:\n\nlm.fit \n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length)\n\nCoefficients:\n (Intercept)  Sepal.Length  \n     3.41895      -0.06188  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1095 -0.2454 -0.0167  0.2763  1.3338 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.41895    0.25356   13.48   &lt;2e-16 ***\nSepal.Length -0.06188    0.04297   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4343 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\n\nRegression minimizes the sum of squared errors (or deviations) from the line. The “errors” are the difference between where Y is, and where Y should be if it followed a perfect line.\nWe can illustrate what this means:\n\nx &lt;- iris$Sepal.Length\ny &lt;- iris$Sepal.Width\nyhat &lt;- predict(lm.fit)\n\nwith(iris, plot(Sepal.Width ~ Sepal.Length))\nwith(iris, abline(lm.fit, col=\"blue\"))\nfor(i in 1:length(x))  lines(x[c(i,i)],c(y[i], yhat[i]), col=\"red\", lty=2)\n\n\n\n\n\n\n\nThe regression line is the best fit line that minimizes the sums of squared deviations from the regression. It turns out that the least-squares fit of the regression line is also provides the Maximum Likelihood fits of the parameters of the line (the slope and intercept)."
  },
  {
    "objectID": "posts/2025-03-06-univariate/index.html#anova",
    "href": "posts/2025-03-06-univariate/index.html#anova",
    "title": "A small review of univariate parametric statistics",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of variance is very closely related to regression. It also works by minimizing sums of squares, but it asks a different question.\nDoes the data better fit a model with one group (one regression line?) or multiple groups (multiple regression lines, one for each group)?\nGraphically, it looks like the plot below with the question Is the data explained better by a single group with a grand mean? or with separate means for each Species?\n\npar(mfrow=c(1,2))\nwith(iris, boxplot(Sepal.Length))\nwith(iris, plot(Sepal.Length ~ Species))\n\n\n\n\n\n\n\nIs One of these Groups not like the others?\nStatistically, this is asking whether the sums of squares is minimized by assuming there is only one group (one mean)? Or three groups?\nFor this plot we will add an index column (1 to number of observations), and use ggplot2, dplyr, and the pipe from magrittr\n\nrequire(dplyr)\nrequire(magrittr)\nrequire(ggplot2)\n\ndat &lt;- cbind(iris, id = 1:length(iris$Species))\nyhat &lt;- mean(iris$Sepal.Length)  # grand mean of Sepal Length\n\np &lt;- dat %&gt;% ggplot(aes( x = id, y = Sepal.Length, group=Species)) \n\nq &lt;- p + geom_point( size=2) + \n  geom_hline( aes(yintercept = mean(iris$Sepal.Length)) ) + \n  geom_segment( data=dat, aes( x = id, y = Sepal.Length, xend = id, yend = yhat), color=\"red\", lty = 3)\n\nq  \n\n\n\n\n\n\n\nWe added two new ggplot2 functions:\n\n\ngeom_hline() which adds a horizontal line used for the grand mean. This is similar to base R abline()\n\n\ngeom_segment() which plots line segments indicated by x,y start points and xend,yend end points (this is based on the base R segment() function)\n\nTo add in the group structure, we need to compute means by species, and know where one species ends and the other begins in the data vector. We can do this with group_by() and summarize():\n\nspmeans  &lt;- dat %&gt;% group_by(Species) %&gt;% \n        summarise(\n          sl = mean(Sepal.Length),\n          n = length(Sepal.Length),\n          minid = min(id),\n          maxid = max(id)\n        )\n\nspmeans\n\n# A tibble: 3 × 5\n  Species       sl     n minid maxid\n  &lt;fct&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 setosa      5.01    50     1    50\n2 versicolor  5.94    50    51   100\n3 virginica   6.59    50   101   150\n\n\n(You should always check that minid and maxid is what you intended)\nWe want to include this mean information in the dataframe, so to add it as a vector, we can merge():\n\ndat &lt;- merge(dat, spmeans)\nhead(dat)\n\n  Species Sepal.Length Sepal.Width Petal.Length Petal.Width id    sl  n minid\n1  setosa          5.1         3.5          1.4         0.2  1 5.006 50     1\n2  setosa          4.9         3.0          1.4         0.2  2 5.006 50     1\n3  setosa          4.7         3.2          1.3         0.2  3 5.006 50     1\n4  setosa          4.6         3.1          1.5         0.2  4 5.006 50     1\n5  setosa          5.0         3.6          1.4         0.2  5 5.006 50     1\n6  setosa          5.4         3.9          1.7         0.4  6 5.006 50     1\n  maxid\n1    50\n2    50\n3    50\n4    50\n5    50\n6    50\n\ndat[45:55,]\n\n      Species Sepal.Length Sepal.Width Petal.Length Petal.Width id    sl  n\n45     setosa          5.1         3.8          1.9         0.4 45 5.006 50\n46     setosa          4.8         3.0          1.4         0.3 46 5.006 50\n47     setosa          5.1         3.8          1.6         0.2 47 5.006 50\n48     setosa          4.6         3.2          1.4         0.2 48 5.006 50\n49     setosa          5.3         3.7          1.5         0.2 49 5.006 50\n50     setosa          5.0         3.3          1.4         0.2 50 5.006 50\n51 versicolor          7.0         3.2          4.7         1.4 51 5.936 50\n52 versicolor          6.4         3.2          4.5         1.5 52 5.936 50\n53 versicolor          6.9         3.1          4.9         1.5 53 5.936 50\n54 versicolor          5.5         2.3          4.0         1.3 54 5.936 50\n55 versicolor          6.5         2.8          4.6         1.5 55 5.936 50\n   minid maxid\n45     1    50\n46     1    50\n47     1    50\n48     1    50\n49     1    50\n50     1    50\n51    51   100\n52    51   100\n53    51   100\n54    51   100\n55    51   100\n\ntail(dat)\n\n      Species Sepal.Length Sepal.Width Petal.Length Petal.Width  id    sl  n\n145 virginica          6.7         3.3          5.7         2.5 145 6.588 50\n146 virginica          6.7         3.0          5.2         2.3 146 6.588 50\n147 virginica          6.3         2.5          5.0         1.9 147 6.588 50\n148 virginica          6.5         3.0          5.2         2.0 148 6.588 50\n149 virginica          6.2         3.4          5.4         2.3 149 6.588 50\n150 virginica          5.9         3.0          5.1         1.8 150 6.588 50\n    minid maxid\n145   101   150\n146   101   150\n147   101   150\n148   101   150\n149   101   150\n150   101   150\n\n\nNow that we have our dataframe with all of the necessary information, we can plot.\nNote that there are two calls to geom_segment(). For the first, we are plotting the species means, so we use the smmeans dataset. For the second, we are plotting each pointʻs deviation from the species means so we use the full dataset. The rest is telling the function where the start and end points of each segment are:\n\nr &lt;- p + geom_point( size=2) + \n  geom_segment( data=spmeans, aes(x=minid, y = sl, xend=maxid, yend=sl, group=Species )) + \n  geom_segment( data=dat, aes( x = id, y = Sepal.Length, xend = id, yend = sl, color=Species), lty = 3) \n\nr  \n\n\n\n\n\n\n\nBack to our question - is the error sum of squares minimized by accouting for separate species or considering all irises as one group? Another way to state ANOVA is - is at least one of these groups different than the others?\n\nrequire(cowplot)\n\nLoading required package: cowplot\n\nplot_grid(\n  q, \n  r + theme(legend.position=\"none\"), \n  labels=\"AUTO\")\n\n\n\n\n\n\n\nIf we want to know whether species are different in sepal length, then we need to have lm fit the model by species. We do this like so:\n\nlm.fit &lt;- with(iris, lm(Sepal.Length ~ Species))\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation: One-way ANOVA is like fitting a regression of the individual points against the grand mean of the points vs. separate regressions for each group. The summary shows that the the intercept (the mean of setosa) is about 5 (significantly different than zero), whereas the other species are contrasts against setosa, the first species. Versicolor is 0.93 higher than setosa, and virginica is 1.58 higher than setosa. Both of these contrasts are signficant. So they are actually all significantly different than each other\nNotice that now have more parameters estimated. You can specify which parameter values and contrasts you want displayed. Often we just want an ANOVA table, which tests the hypothesis that at least one group is different than the others:\n\nanova(lm.fit)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see that species are significantly different in sepal length. Can you make a plot that shows this and add the statistics to it?\nAnd thatʻs how ANOVA is related to regression!"
  },
  {
    "objectID": "posts/2025-03-06-univariate/index.html#ancova",
    "href": "posts/2025-03-06-univariate/index.html#ancova",
    "title": "A small review of univariate parametric statistics",
    "section": "ANCOVA",
    "text": "ANCOVA\nThere are many forms of regession and ANOVA. For example, if you want to see if the relationship between Sepal.Length and Sepal.Width differs by species, you woul do an ANCOVA (analysis of covariance):\n\nlm.fit &lt;- with(iris, lm(Sepal.Width ~ Sepal.Length + Species))\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + Species)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95096 -0.16522  0.00171  0.18416  0.72918 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.67650    0.23536   7.123 4.46e-11 ***\nSepal.Length       0.34988    0.04630   7.557 4.19e-12 ***\nSpeciesversicolor -0.98339    0.07207 -13.644  &lt; 2e-16 ***\nSpeciesvirginica  -1.00751    0.09331 -10.798  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.289 on 146 degrees of freedom\nMultiple R-squared:  0.5693,    Adjusted R-squared:  0.5604 \nF-statistic: 64.32 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nWhich would fit separate Y-intercepts for each species."
  },
  {
    "objectID": "posts/2025-01-30-reference-management/index.html",
    "href": "posts/2025-01-30-reference-management/index.html",
    "title": "Reference management",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by skimming the following materials:\n\nR Markdown cheatsheet\n\nBibliographies and Citations in pandoc\n\nCitations from Reproducible Research in R from the Monash Data Fluency initiative\nCitation style, appendix, etc. R Markdown Cookbook\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/\nhttps://andreashandel.github.io/MADAcourse\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\nhttps://monashdatafluency.github.io/r-rep-res/citations.html"
  },
  {
    "objectID": "posts/2025-01-30-reference-management/index.html#the-parts",
    "href": "posts/2025-01-30-reference-management/index.html#the-parts",
    "title": "Reference management",
    "section": "The Parts",
    "text": "The Parts\nThere are three basic parts:\n\nThe citation data is stored in the .bib file Section 4, and\n\nIn-text citations added by the author into the .qmd document Section 7, and\n\nLinking the ʻ.bibʻ file in the YAML header Section 6.\n\nFrom this, both the in-text citations as well as the citation list at the end of the document will be rendered.\nThere are additional customization options. You can change the style of the bibliography using style and class files Section 8. This is at the level of the entire document, it is easy to switch. Within the document, there are also many options for in-text citation styles Section 9. You can also customize the style file by editing the LaTeX.\nReference managers are helper software (independent of BibTeX) that are wonderful tools to help you collect and organize your citation data Section 5."
  },
  {
    "objectID": "posts/2025-01-30-reference-management/index.html#citation-management-software",
    "href": "posts/2025-01-30-reference-management/index.html#citation-management-software",
    "title": "Reference management",
    "section": "Citation management software",
    "text": "Citation management software\nIn addition to .bib (BibTeX) there are a lot of file formats in use including .medline (MEDLINE), .ris (RIS), and .enl (EndNote), among others. You can generally download the results of your literature search in the format of your choice (some citation manager software can convert formats as well).\nIf you recall the output from citation(\"rmarkdown\") above, one option is to copy and paste the BibTeX output into a text file labeled .bib or into citation management software, but instead we can use Rʻs write_bib() function from the knitr package to create a bibliography file.\nLet’s run the following code in order to generate a my-refs.bib file\n\nknitr::write_bib(\"rmarkdown\", file = \"my-refs.bib\")\n\nYou can output multiple citations by passing a vector of package names:\n\nknitr::write_bib(c(\"rmarkdown\",\"base\"), file = \"my-refs.bib\")\n\nNow we can see we have the file saved locally.\n\nlist.files()\n\n[1] \"ButlerPapers.bib\" \"evolution.csl\"    \"index.html\"       \"index.qmd\"       \n[5] \"index.rmarkdown\"  \"my-refs.bib\"     \n\n\nIf you open up the my-refs.bib file, you will see\n@Manual{R-base,\n  title = {R: A Language and Environment for Statistical Computing},\n  author = {{R Core Team}},\n  organization = {R Foundation for Statistical Computing},\n  address = {Vienna, Austria},\n  year = {2022},\n  url = {https://www.R-project.org/},\n}\n\n@Manual{R-rmarkdown,\n  title = {rmarkdown: Dynamic Documents for R},\n  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},\n  year = {2021},\n  note = {R package version 2.8},\n  url = {https://CRAN.R-project.org/package=rmarkdown},\n}\n\n@Book{rmarkdown2018,\n  title = {R Markdown: The Definitive Guide},\n  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2018},\n  note = {ISBN 9781138359338},\n  url = {https://bookdown.org/yihui/rmarkdown},\n}\n\n@Book{rmarkdown2020,\n  title = {R Markdown Cookbook},\n  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2020},\n  note = {ISBN 9780367563837},\n  url = {https://bookdown.org/yihui/rmarkdown-cookbook},\n}\n\nNote there are three keys that we will use later on:\n\nR-rmarkdown\nrmarkdown2018\nrmarkdown2020"
  },
  {
    "objectID": "posts/2025-02-13-data/index.html",
    "href": "posts/2025-02-13-data/index.html",
    "title": "Types of Data",
    "section": "",
    "text": "Watch\n\n\n\nBefore class, you can prepare by watching:\n\n\n\n✏️"
  },
  {
    "objectID": "posts/2025-02-13-data/index.html#acknowledgements",
    "href": "posts/2025-02-13-data/index.html#acknowledgements",
    "title": "Types of Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://andreashandel.github.io/MADAcourse/Data_Types.html\nhttps://r-coder.com/data-types-r/#Raw_data_type_in_R\n\nhttps://www.stat.auckland.ac.nz/~paul/ItDT/HTML/node76.html # Learning objectives\n\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand different types of data and how they are represented computationally\nUnderstand that different data types require different analysis approaches\nRecognize different base data types in R and know how to work with them\nRecognize the base data structures or objects in R and how to use them to do what you want"
  },
  {
    "objectID": "posts/2025-02-13-data/index.html#basic-data-types",
    "href": "posts/2025-02-13-data/index.html#basic-data-types",
    "title": "Types of Data",
    "section": "Basic data types",
    "text": "Basic data types\n\n\n\n\n\n\nR has six basic (atomic) types of data:\n\n\n\n\n\nAtomic Type\nShort Description\nSize in bytes\n\n\n\n\nstring (or character)\ntext\n1 (per character)\n\n\ninteger\ncountable numbers\n4\n\n\nreal\nreal numbers\n8\n\n\nlogical\nTRUE or FALSE\n4\n\n\ncomplex\nnumbers with imaginary component\n\n\n\nraw\nraw bytes\n1\n\n\n\nAll other data types are derived from the atomic types\n\n\nNote: Most computers use 64-bit operating systems these days, so the sizes above are for 64-bit software.\nFor a quick tour of the data types, see https://r-coder.com/data-types-r\nString/character: Character values are alphanumeric values (plus whitespace, punctuation, etc.). A string is a collection of characters, in other words “text”.\n\nstrings can be pasted together using the paste() function.\nR has powerful tools for string manipulation, including searching, replacing, and customized partial matching (with or without replacement) using wildcards and perl-like regular expressions (or regex) using base functions such as\n\ngrep()\nsub()\ngsub()\nsubstr()\n\n\nThere are also packages specific for string manipulation including the stringr package which is part of the tidyverse.\n\nIt is very likely that you will need to work with strings at some point during a data analysis, even if it is only to find specific values, clean up variable names, etc.\nThese problems can be quite the headache! But instead of editing them by hand and possibly making an error, it is better to do this with code. It also makes it easier to keep a record of the original data and all of the changes made to it, improving the reproducibility of your analysis.\nThere is a learning curve to using these tools, especially regex syntax, but they are very powerful and well worth your time.\n\n\n\n\n\n\nGood sources for practice manipulating strings:\n\n\n\n\nFor beginners: Review the Strings chapter (14) of R4DS, and do the exercises.\nThe string processing chapter (25) of IDS\n\nthe Character Vectors chapter in the STAT 545 book by Jenny Bryan\n\n\nDecide which one is right for your level and work through some examples. I think youʻll agree that it is worth your time.\n\n\nNumeric (double or integer): Variables of type numeric in R are either integers or double precision (representing real numbers).\n\nIntegers and real values are different, but in practice most R users donʻt pay attention to this distinction. Integer values tend to be coerced (converted) to real values if any mathematical operations are done to them.\nIf an integer is explicity needed, you can create them using functions such as as.integer().\nNote that when you type an integer value, e.g. x &lt;- 2, into R, this is considered numeric by default.\nIf you want to make sure a value is treated as integer, add an L, e.g. x &lt;- 2L.\n\nLogical: Logical variables are binary and can take on only two values, TRUE or FALSE (which are reserved words that only take on these meanings in R).\n\nIn R, logical values are treated as integers, and interpreted as 1 for TRUE and 0 for FALSE. It is possible to sum(TRUE) or a vector of logicals, for example.\n\nR also understands T, True, and true for TRUE, and the corresponding representations for FALSE.\nImportantly, logical comparisons are used for indexing. You will use logical comparisons when cleaning and checking your data, or running analyses, e.g., if you want to see if your variable x is greater than 5, then the R command x &gt; 5 will return either TRUE or FALSE, based on the value of x.\n\nNote: reserved words are understood as constants and should not be “quoted”."
  },
  {
    "objectID": "posts/2025-02-13-data/index.html#derived-data-types",
    "href": "posts/2025-02-13-data/index.html#derived-data-types",
    "title": "Types of Data",
    "section": "Derived data types",
    "text": "Derived data types\nR also allows derived data types called classes that are built up from atomic data types. There are R base classes as well as new classes that can be defined as needed by programmers (maybe you?).\nFactors: Are Rʻs class for categorical variables.\n\n\nFactors have names and values.\nFor example, a size factor may have names (or levels) of small, medium. and large with values 0,1,2. Here, the values simply indicate the different categories, with the names being the human-friendly labels for the values.\n\nFactors can be ordered/ordinal or not.\nFactors could be numeric values, e.g., the number of offspring.\nOr it could be a factor coding for 3 types of habitat (unordered),\nOr 3 levels of life history stage (ordered).\nAn excellent package to work with factors is the forcats package.\n\nFor more about factors, work through the Factors chapter of R4DS, and do the exercises.\nDate/time: Dates in base R are of the class Date (and are called POSIX variables). The lubridate package is a tidyverse package to work with dates, which many people find easier. There are other packages as well.\nAdditional resources are the Dates and times chapter of R4DS and the Parsing Dates and Times chapter of IDS.\nProgrammer-defined classes Many packages define their own classes. For example class phylo is used to represent phylogenetic trees in the ape package.\nThere are several functions that can show you the data type of an R object\nsuch as typeof(), mode(), storage.mode(), class() and str()."
  },
  {
    "objectID": "posts/2025-02-13-data/index.html#other-derived-data-types",
    "href": "posts/2025-02-13-data/index.html#other-derived-data-types",
    "title": "Types of Data",
    "section": "Other derived data types",
    "text": "Other derived data types\nTimeseries: A very useful set of tools for times-series analysis in R is the set of packages called the tidyverts. CRAN also has a Task View for Time Series Analysis. (A Task View on CRAN is a site that tries to combine and summarize various R packages for a specific topic). Another task view that deals with longitudinal/time-series data is the Survival Analysis Task View.\nOmics: The bioconductor website is your source for (almost) all tools and resources related to omics-type data analyses in R.\nText: Working with and analyzing larger sections of text is different from the simple string manipulation discussed above. These days, analysis of text often goes by the term natural language processing. Such text analysis will continue to increase in importance, given the increasing data streams of that type. If you are interested in doing full analyses of text data, the tidytext R package and the Text mining with R book are great resources. A short introduction to this topic is The text mining chapter (27) of IDS.\nImages: Images are generally converted into multiple matrices of values for different pixels of an image. For instance, one could divide an image into a 100x100 grid of pixels, and assign each pixel a RGB values and intensity. That means one would have 4 matrices of numeric values, each of size 100x100. One would then perform operations on those values. We won’t do anything with images here, there are some R packages for analyzing image data.\nVideos: Are a time-series of images. Analysis of videos therefore has an extra layer of complexity."
  },
  {
    "objectID": "posts/2025-02-13-data/index.html#for-sequences",
    "href": "posts/2025-02-13-data/index.html#for-sequences",
    "title": "Types of Data",
    "section": "For sequences",
    "text": "For sequences\n\n\nFunctions\nActions\n\n\n\nseq()\ngenerate a sequence of numbers\n\n\n1:10\nsequence from 1 to 10 by 1\n\n\nrep(x, times)\nreplicates x\n\n\nsample(x, size, replace=FALSE)\nsample size elements from x\n\n\nrnorm(n, mean=0, sd=1)\ndraw n samples from normal distribution"
  },
  {
    "objectID": "posts/2025-02-13-data/index.html#creating-or-coercing-objects-to-different-class",
    "href": "posts/2025-02-13-data/index.html#creating-or-coercing-objects-to-different-class",
    "title": "Types of Data",
    "section": "Creating or Coercing objects to different class",
    "text": "Creating or Coercing objects to different class\n\n\nFunctions\nActions\n\n\n\nvector()\ncreate a vector\n\n\nmatrix()\ncreate a matrix\n\n\ndata.frame()\ncreate a data frame\n\n\nas.vector(x)\ncoerces x to vector\n\n\nas.matrix(x)\ncoerces to matrix\n\n\nas.data.frame(x)\ncoerces to data frame\n\n\nas.character(x)\ncoerces to character\n\n\nas.numeric(x)\ncoerces to numeric\n\n\nfactor(x)\ncreates factor levels for elements of x\n\n\nlevels()\norders the factor levels as specified"
  },
  {
    "objectID": "posts/2025-02-13-data/index.html#footnotes",
    "href": "posts/2025-02-13-data/index.html#footnotes",
    "title": "Types of Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nFor example, logistic regression (a regression to predict a binary [yes/no] outcome) is used for classification. The underlying model predicts a quantitative outcome (a value between 0 and 1 usually interpreted as a probability), which is then binned to make categorical predictions.↩︎"
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html",
    "href": "posts/2025-02-04-reading-data/index.html",
    "title": "Data IO",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttp://rafalab.dfci.harvard.edu/dsbook/importing-data.html\nhttps://www.stephaniehicks.com/jhustatcomputing2022"
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html#get-the-files-onto-your-computer",
    "href": "posts/2025-02-04-reading-data/index.html#get-the-files-onto-your-computer",
    "title": "Data IO",
    "section": "Get the files onto your computer",
    "text": "Get the files onto your computer\nFirst, get the data files from the rclassdata GitHub repo by cloning the repo to your Documents/git/rclass folder.\nOpen R by double clicking on an R script within your rclassdata folder. Which working directory are you in? If necessary, use setwd() to get move into the rclassdata folder.\n\ngetwd()\nsetwd(\"~/Documents/git/rclass/rclassdata\")  \nlist.files()  # will tell us which files are in this folder"
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html#read.csv",
    "href": "posts/2025-02-04-reading-data/index.html#read.csv",
    "title": "Data IO",
    "section": "read.csv",
    "text": "read.csv\nGetting the file into R is easy. If it is in csv format, you just use:\n\nread.csv(\"anolisSSD.csv\")  # look for the file in the working directory\n\nThis is an Anolis lizard sexual size dimorphism dataset. It has values of dimorphism by species for different ecomorphs, or microhabitat specialists.\nTo save the data as an R object, give it a name and save it:\n\nanolis &lt;- read.csv(\"anolisSSD.csv\")\n\nIt is a good practice to always check that the data were read in properly. If it is a large file, you’ll want to at least check the beginning and end were read in properly:\n\nhead(anolis)\n\n  species   logSSD    ecomorph\n1      oc -0.00512        twig\n2      eq  0.08454 crown-giant\n3      co  0.24703 trunk-crown\n4     aln  0.24837 trunk-crown\n5      ol  0.09844  grass-bush\n6      in  0.06137        twig\n\ntail(anolis)\n\n   species  logSSD     ecomorph\n18      cr 0.39796 trunk-ground\n19      st 0.15737  trunk-crown\n20      cy 0.26024 trunk-ground\n21     alu 0.08216   grass-bush\n22      lo 0.13108        trunk\n23      an 0.13547         twig\n\n\nWhich prints out the first six and last six lines of the file.\nVoila! Now you can plot, take the mean, etc. For example:\n\nplot(as.factor(anolis$ecomorph), anolis$logSSD)\n\n\n\n\n\n\n\n(Aside) You probably didn’t understand the line of code above, but we will get into this very soon in the next lectures. But you can break it down bit by bit.\nNow try reading in anolisSSDsemicolon.csv what did you get? Try reading it in with read.table() - check out the help page.\nR can read in many other formats as well, including database formats, excel native format (although it is easier in practice to save as .csv), fixed width formats, and scanning lines. For more information see the R manual “R Data Import/Export” which you can get from help.start() or at http://www.r-project.org."
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html#input-files-generated-by-data-loggers",
    "href": "posts/2025-02-04-reading-data/index.html#input-files-generated-by-data-loggers",
    "title": "Data IO",
    "section": "Input files generated by data loggers",
    "text": "Input files generated by data loggers\nFiles that are generated by computer, even if they are not separated by commas (.csv) are not too bad to deal with. Take, for example, the file format generated from our hand-held Ocean Optics specroradiometer. It is very regular in structure, and we have tons of data files, so it is well worth the programming effort to code a script for automatic file input.\nFirst, you can open the file below in a text editor. If you’d rather open it in R, you can use:\n\nreadLines(\"20070725_01forirr.txt\")\n\nNotice that there is a very large header, in fact the first 17 lines. Notice also that the last line will cause a problem. Also, the delimiter in this file is tab (backslash t).\n\ntemp &lt;- readLines(\"20070725_01forirr.txt\")\nhead(temp)\ntail(temp)\n\nWe can solve these issues using the skip and the comment.char arguments of read.table to ignore both types of lines, reading in only the “good stuff”. Also, the default delimiter in this function is the tab:\n\ndat &lt;- read.table(file=\"20070725_01forirr.txt\", skip=17, comment.char=\"&gt;\")\nnames(dat) &lt;- c(\"lambda\", \"intensity\")\nhead(dat)\n\n  lambda intensity\n1 177.33         0\n2 177.55         0\n3 177.77         0\n4 177.99         0\n5 178.21         0\n6 178.43         0\n\ntail(dat)\n\n     lambda intensity\n3643 888.21   0.29491\n3644 888.38   0.31306\n3645 888.54   0.28153\n3646 888.71   0.28245\n3647 888.87   0.18988\n3648 889.04   0.18988\n\n\nThe file produces (useless) rows of data outside of the range of accuracy of the spectraradiometer. We can get rid of these by subsetting the data, selecting only the range 300-750nm:\n\ndat &lt;- dat[dat$lambda &gt;= 300, ]  # cut off rows below 300nm\ndat &lt;- dat[dat$lambda &lt;= 750, ]  #cut off rows above 750nm\n\nOr do both at once:\n\ndat &lt;- dat[dat$lambda &gt;= 300 & dat$lambda &lt;= 750,]\n\nIf we are going to be doing this subsetting over and over, we might want to save this as an index vector which tells us the position of the rows of data we want to keep in the dataframe (don’t worry, we’ll cover this again in the workhorse functions chapter).\n\noo &lt;- dat$lambda &gt;= 300 & dat$lambda &lt;= 750\ndat &lt;- dat[oo, ]   # same as longer version above\n\nWe can now save the cleaned up version of the irradiance data:\n\nwrite.csv(dat, \"20070725_01forirr.csv\")"
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html#human-error",
    "href": "posts/2025-02-04-reading-data/index.html#human-error",
    "title": "Data IO",
    "section": "Human Error",
    "text": "Human Error\nIt is very easy to make a typo, and humans are really bad at catching our own typos in real time. Right? They creep in despite our best efforts. This is why even though it is possible to measure specimens and enter the numbers directly into the computer, itʻs not something I would do.\nI always write my measurements down into a notebook or a datasheet using pencil and paper. This has saved me many errors in three ways (1) sometimes my brain is still processing what I just wrote and I will catch a typo. (2) I write my data in rows for specimens and columns for the different measurements. As the dataset builds, it is easy to notice errors if some number is really off. (3) If you have any doubts, you can quickly look over your page and re-measure anything suspicous. If you have the person-power, you can also have one person taking the measurements and calling them out, and another writing them down and repeating them back. It is a very effective way to check on the spot.\nI had my first post-college job at an insurance rating board. This is a business that deals with reams and reams of data. We did work a lot with Fortran code and spreadsheets, but surprisingly, some of it does actually have to get manually checked. The protocol was simple, a paper printout of the old version was put next to the new version and a person went along with a ruler, literally putting a check mark after verifying the number. A second check was done as well, and then I finally understood the meaning of “double checking” 😝. When I got to grad school, it was eye-opening to find that checking any personʻs data entry was a rare practice 😲. Please, whenever possible check your data entry.\n\n\n\n\n\n\nHere are some tips:\n\n\n\n\n\nRecord data in a notebook (paper and pencil). It serves as a permanent record.\nWrite it down in a data table format in an order that minimizes error. For example, if it is convenient to take five measurements in a particular order, then organize your table that way and always take the measurements in the same order.\n\nOrganize your spreadsheet to mirror the hand-written data. This will minimize data entry errors.\n\nHave another person check your data entry against the notebook (data source)."
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html#organizing-your-spreadhseet",
    "href": "posts/2025-02-04-reading-data/index.html#organizing-your-spreadhseet",
    "title": "Data IO",
    "section": "Organizing your spreadhseet",
    "text": "Organizing your spreadhseet\nWhen it comes time to enter your data in a spreadsheet, there are many things you can do to improve organization. Below is a summary of the recommendations made in paper by Karl Broman and Kara Woo (Broman and Woo 2018).\n\n\nBe Consistent - Have a plan before you start entering data. Be consistent and stick to it.\n\nChoose Good Names for Things - You want the names you pick for objects, files, and directories to be memorable, easy to spell, descriptive, but concise. This is actually a hard balance to achieve and it does require time and thought.\n\nOne important rule to follow is do not use spaces, use underscores _ or dashes instead -.\nAlso, avoid symbols; stick to letters and numbers.\n\n\n\nWrite Dates as YYYY-MM-DD - To avoid confusion, we strongly recommend using this global ISO 8601 standard.\n\nNo Empty Cells - Fill in all cells and use some common code for missing data.\n\nPut Just One Thing in a Cell - It is better to add columns to store the extra information rather than having more than one piece of information in one cell.\n\nMake It a Rectangle - The spreadsheet should be a rectangle.\n\nCreate a Data Dictionary - If you need to explain things, such as what the columns are or what the labels used for categorical variables are, do this in a separate file. This is an excellent use for a README.md file\n\nNo Calculations in the Raw Data Files - Excel permits you to perform calculations. Do not make this part of your spreadsheet. Code for calculations should be in a script.\n\nDo Not Use Font Color or Highlighting as Data - Most import functions are not able to import this information. Encode this information as a variable (a “comment” column) instead.\n\nMake Backups - Make regular backups of your data.\n\nUse Data Validation to Avoid Errors - Leverage the tools in your spreadsheet software so that the process is as error-free and repetitive-stress-injury-free as possible. Think of checks you can do for “reality checks”.\n\nSave the Data as Text Files - Save files for sharing in comma or tab delimited format. An unambiguous text format is the best for archiving your data."
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html#text-versus-binary-files",
    "href": "posts/2025-02-04-reading-data/index.html#text-versus-binary-files",
    "title": "Data IO",
    "section": "Text versus binary files",
    "text": "Text versus binary files\nFor data science purposes, files can generally be classified into two categories: text files (also known as ASCII files) and binary files. You have already worked with text files. All your R scripts are text files and so are the Quarto files used to create this website. The .csv tables you have read are also text files. One big advantage of these files is that we can easily “look” at them using a plain text editor, without having to purchase any kind of special software.\nAny text editor can be used to examine a text file, including freely available editors such as R, RStudio, Atom, Notepad, TextEdit, vi, emacs, nano, and pico. However, if you try to open, say, an Excel xls file, jpg or png file, you will not be able to see anything immediately useful. These are binary files. Excel files are actually compressed folders with several text files inside. But the main distinction here is that text files can be easily examined.\nAlthough R includes tools for reading widely used binary files, such as xls files, in general you will want to find data sets stored in text files. If necessary, use the proprietary software to export the data into text files and go from there.\n\n\n\n\n\n\nWarning\n\n\n\nThe problem with using propietary formats for data management is that htey are poor formats for achival purposes. These formats may change or they donʻt copy well, so that eventually you can no longer open them.\n\n\nSimilarly, when sharing data you want to make it available as text files as long as storage is not an issue (binary files are much more efficient at saving space on your disk). In general, plain-text formats make it easier to share data since commercial software is not required for working with the data, and they are more reliable.\nTechnically, html and xml files are text files too, but they have complicated tags around the information. In the Data Wrangling part of the book we learn to extract data from more complex text files such as html files."
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html#unicode-versus-ascii",
    "href": "posts/2025-02-04-reading-data/index.html#unicode-versus-ascii",
    "title": "Data IO",
    "section": "Unicode versus ASCII",
    "text": "Unicode versus ASCII\nA pitfall in data science is assuming a file is an ASCII text file when, in fact, it is something else that can look a lot like an ASCII text file, for example, a Unicode text file.\nTo understand the difference between these, remember that everything on a computer needs to eventually be converted to 0s and 1s (binary format). ASCII is an encoding that maps bits to characters that are easier for humans to read.\n\n\n\n\n\n\nBinary data can take on only two values - 0 or 1\n\n\n\n\na bit is the smallest unit, a single binary character (0 or 1).\na byte is eight bits.\na megabyte or MB is one million bytes.\na gigabyte or GB is one billion bytes.\nYou can roughly calculate the size of your data by the numbers of bytes per each observation.\n\n\n\nASCII uses 7 bits – seven variables that can be either 0 or 1 – which results in 27 = 128 unique items, enough to encode all the characters on an English language keyboard (all characters, numbers, and symbols, Figure 1, Figure 2). However, we need to expand the possibilities if we want to include support for other languages or additional characters.\n\n\n\n\n\nFigure 1: Binary encodings in ACII, with some of the most used escape sequences like \\t for tab and \\n for new line.\n\n\n\n\n\n\n\nFigure 2: Some examples of ASCII encodings for A-E and a-e.\n\n\nFor this reason, a new encoding, using more than 7 bits, was defined: Unicode. When using Unicode, one can chose between 8, 16, and 32 bits abbreviated UTF-8, UTF-16, and UTF-32 respectively. RStudio actually defaults to UTF-8 encoding.\nAlthough we do not go into the details of how to deal with the different encodings here, it is important that you know these different encodings exist so that you can better diagnose a problem if you encounter it. One way problems manifest themselves is when you see “weird looking” characters you were not expecting.\nMany plain text editors (Atom, Sublime, TextWrangler, Notepad++) will detect encodings and tell you what they are and may also convert between them. Also from the command line the file command will reveal the encoding (should also work on Windows if you have git installed:\n\n\nTerminal\n\nfile filename\n\nThis StackOverflow discussion is an example: https://stackoverflow.com/questions/18789330/r-on-windows-character-encoding-hell."
  },
  {
    "objectID": "posts/2025-02-04-reading-data/index.html#line-endings",
    "href": "posts/2025-02-04-reading-data/index.html#line-endings",
    "title": "Data IO",
    "section": "Line endings",
    "text": "Line endings\nOne last potential headache is the character used for line endings. The line ending is the invisible (to us) character that is added to your file when you press the return key.\nIt is all one stream of informatin to the computer, but the computer will interpret the information as a new line when it detects one of these characters. When software is provided a file with an unexpected line ending, it is not able to properly detect the lines of information (maybe it sees only one huge line). See?\nThere are two types of line endings in use today:\n\nOn UNIX and MacOS, text file line-endings are terminated with a newline character (ASCII 0x0a, represented by the \\n escape sequence in most languages), also referred to as a linefeed (LF).\nOn Windows, line-endings are terminated with a combination of a carriage return (ASCII 0x0d or \\r) and a newline(\\n), also referred to as CR/LF.\n\nIf your computer complains about line endings, the easiest thing to do is to open it in one of the good plain text editors and save it with the line endings it is expecting (usually as LF instead of CR/LF)."
  },
  {
    "objectID": "posts/2025-02-27-dplyr/index.html",
    "href": "posts/2025-02-27-dplyr/index.html",
    "title": "Getting data in shape with dplyr",
    "section": "",
    "text": "Pre-lecture materials\n\n🌴\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://r4ds.had.co.nz/tibbles\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-06-managing-data-frames-with-tidyverse/\nhttps://rdpeng.github.io/Biostat776/lecture-managing-data-frames-with-the-tidyverse\nhttps://jhudatascience.org/tidyversecourse/get-data.html#tibbles\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand the tools available to get data into the proper structure and shape for downstream analyses\nLearn about the dplyr R package to manage data frames\nRecognize the key verbs (functions) to manage data frames in dplyr\nUse the “pipe” operator to combine verbs together\n\n\n\nOverview\nIt is still important to understand base R manipulations, particularly for things such as cleaning raw data, troubleshooting, and writing custom functions. But the tidyverse provides many useful tools for data manipuation and analysis of cleaned data. In this session we will learn about dplyr and friends.\nTidy data\nThe tidyverse has many slogans. A particularly good one for all data analysis is the notion of tidy data.\nAs defined by Hadley Wickham in his 2014 paper published in the Journal of Statistical Software, a tidy dataset has the following properties:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\nArtwork by Allison Horst on tidy data\n\n[Source: Artwork by Allison Horst]\nWhat shapes does the data need to be in?\nBeyond the data being tidy, however, we also need to think about what shape it needs to be in. Weʻll review concepts and tools in the next two lessons.\nNow that we have had some experience plotting our data, we can see the value of having rectangular dataframes. We can also see that for particular graphics and analyses, we need to have the data arranged in particular ways.\nFor example, take a look at this elegant graphic below. This single graphic is packed with information on fat, BMR, TEE, and activity levels, all for mulitple species. Is it more effective that individual bar plots? This arrangement is so helpful because you can imagine questions that can be answered with it by comparing the different aspects of the data.\n\n\n\n\n\n\nA very informative figure!\n\n\n\nSource: Gibbons, 2022 based on data of H. Ponzer et al., NATURE, 533:90, 2016\n\n\n\n\n\n\n\n\nCan you imagine what this dataset looks like in terms of organization?\n\n\n\n\nFirst imagine what it would look like variable by variable.\nHow might you intially plot the data?\nWhat organization would you need to make a single figure such as this?\n\n\n\nWe often do not know exactly what we need at the start of a data analysis. We have to play around with different data structures, rearrange the data, look for interesting plots to try, rerrange to fit the input requirements of new functions weʻve discovered, and so on.\nTibbles\nThe tidyverse uses as its central data structure, the tibble or tbl_df. Tibbles are a variation on data frames, claimed to be lazy and surly:\n\nThey don’t change variable names or types when you construct the tibble.\nDon’t convert strings to factors (the default behavior in data.frame()).\nComplain more when a variable doesnʻt exist.\nNo row.names() in a tibble. Instead, you must create a new variable.\nDisplay a different summary style for its print() method.\nAllows non-standard R names for variables\nAllows columns to be lists.\n\nHowever, most tidyverse functions also work on data frames. Itʻs up to you.\n\ntibble() constructor\nJust as with data frames, there is a tibble() constructor function, which functions in many ways with similar syntax as the data.frame() constructor.\nIf you havenʻt already done so, install the tidyverse:\n\ninstall.packages(\"tidyverse\")\n\n\nrequire(tibble)\n\nLoading required package: tibble\n\ntibble( iris[1:4,] )  # the first few rows of iris\n\n# A tibble: 4 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n\nx &lt;- 1:3\ntibble( x, x * 2 )  # name assigned at construction\n\n# A tibble: 3 × 2\n      x `x * 2`\n  &lt;int&gt;   &lt;dbl&gt;\n1     1       2\n2     2       4\n3     3       6\n\nsilly &lt;- tibble(      # an example of a non-standard names\n  `one - 3` = 1:3,  # name = value syntax\n  `12` = \"numeric\",\n  `:)` = \"smile\",\n)\nsilly\n\n# A tibble: 3 × 3\n  `one - 3` `12`    `:)` \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;\n1         1 numeric smile\n2         2 numeric smile\n3         3 numeric smile\n\n\n\nas_tibble() coersion\nas_tibble() converts an existing object, such as a data frame or matrix, into a tibble.\n\nas_tibble( iris[1:4,] )  # coercing a dataframe to tibble\n\n# A tibble: 4 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n\n\nAs output\nMost often we will get tibbles returned from tidyverse functions such as read_csv() from the readr package.\nThe dplyr package\nThe dplyr package, which is part of the tidyverse was written to supply a grammar for data manipulation, with verbs for the most common data manipulation tasks.\n\n\nArtwork by Allison Horst on the dplyr package\n\n[Source: Artwork by Allison Horst]\n\ndplyr functions\n\nselect(): return a subset of the data frame, using a flexible notation\nfilter(): extract a subset of rows from a data frame using logical conditions\narrange(): reorder rows of a data frame\nrelocate(): rearrange the columns of a data frame\nrename(): rename columns in a data frame\nmutate(): add new columns or transform existing variables\nsummarize(): generate summary statistics of the variables in the data frame, by strata if data are hierarchical\n%&gt;%: the “pipe” operator (from magrittr) connects multiple verbs together into a data wrangling pipeline (kind of like making a compound sentence)\n\nNote: Everything dplyr does could already be done with base R. What is different is a new syntax, which allows for more clarity of the data manipulations and the order, and perhaps makes the code more readable.\nInstead of the nested syntax, or typing the dataframe name over and over, we can pipe one operation into the next.\nAnother useful contribution is that dplyr functions are very fast, as many key operations are coded in C++. This will be important for very large datasets or repeated manipulations (say in a simulation study).\n\nstarwars dataset\nWe will use the starwars dataset included with dplyr. You should check out the help page for this dataset ?starwars.\nLetʻs start by using the skim() function to check out the dataset:\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nclass(starwars)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nskimr::skim(starwars)\n\n\nData summary\n\n\nName\nstarwars\n\n\nNumber of rows\n87\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nlist\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nname\n0\n1.00\n3\n21\n0\n87\n0\n\n\nhair_color\n5\n0.94\n4\n13\n0\n11\n0\n\n\nskin_color\n0\n1.00\n3\n19\n0\n31\n0\n\n\neye_color\n0\n1.00\n3\n13\n0\n15\n0\n\n\nsex\n4\n0.95\n4\n14\n0\n4\n0\n\n\ngender\n4\n0.95\n8\n9\n0\n2\n0\n\n\nhomeworld\n10\n0.89\n4\n14\n0\n48\n0\n\n\nspecies\n4\n0.95\n3\n14\n0\n37\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\nfilms\n0\n1\n24\n1\n7\n\n\nvehicles\n0\n1\n11\n0\n2\n\n\nstarships\n0\n1\n16\n0\n5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nheight\n6\n0.93\n174.60\n34.77\n66\n167.0\n180\n191.0\n264\n▂▁▇▅▁\n\n\nmass\n28\n0.68\n97.31\n169.46\n15\n55.6\n79\n84.5\n1358\n▇▁▁▁▁\n\n\nbirth_year\n44\n0.49\n87.57\n154.69\n8\n35.0\n52\n72.0\n896\n▇▁▁▁▁\n\n\n\n\n\nSelecting columns with select()\n\n\n\n\n\n\n\nExample: Suppose we wanted to take the first 3 columns only\n\n\n\nThere are a few ways to do this. We could use numerical indices:\n\nnames(starwars)[1:3]\n\n[1] \"name\"   \"height\" \"mass\"  \n\n\nBut we can also use the names directly:\n\ndat &lt;- select(starwars, c(name, sex:species))\nhead(dat)\n\n# A tibble: 6 × 5\n  name           sex    gender    homeworld species\n  &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  \n1 Luke Skywalker male   masculine Tatooine  Human  \n2 C-3PO          none   masculine Tatooine  Droid  \n3 R2-D2          none   masculine Naboo     Droid  \n4 Darth Vader    male   masculine Tatooine  Human  \n5 Leia Organa    female feminine  Alderaan  Human  \n6 Owen Lars      male   masculine Tatooine  Human  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe : normally cannot be used with names or strings, but inside the select() function you can use it to specify a range of variable names.\n\n\nBy exclusion\nVariables can be omited using the negative sign withing select():\n\nselect( starwars, -(sex:species))\n\nThe select() function also has several helper functions that allow matching on patterns. So, for example, if you wanted to keep every variable that ends with “color”:\n\ndat &lt;- select(starwars, ends_with(\"color\"))\nstr(dat)\n\ntibble [87 × 3] (S3: tbl_df/tbl/data.frame)\n $ hair_color: chr [1:87] \"blond\" NA NA \"none\" ...\n $ skin_color: chr [1:87] \"fair\" \"gold\" \"white, blue\" \"white\" ...\n $ eye_color : chr [1:87] \"blue\" \"yellow\" \"red\" \"yellow\" ...\n\n\nOr all variables that start with n or m:\n\ndat &lt;- select(starwars, starts_with(\"n\") | starts_with(\"m\"))\nstr(dat)\n\ntibble [87 × 2] (S3: tbl_df/tbl/data.frame)\n $ name: chr [1:87] \"Luke Skywalker\" \"C-3PO\" \"R2-D2\" \"Darth Vader\" ...\n $ mass: num [1:87] 77 75 32 136 49 120 75 32 84 77 ...\n\n\nYou can also use more general regular expressions. See the help page (?select) for more details.\nSubsetting with filter()\n\nThe filter() function is used to extract subsets of rows or observations from a data frame. This function is similar to the existing subset() function in base R, or indexing by logical comparisons.\n\n\nArtwork by Allison Horst on filter() function\n\n[Source: Artwork by Allison Horst]\n\n\n\n\n\n\nExample\n\n\n\nSuppose we wanted to extract the rows of the starwars data frame where the birthyear is greater than 100:\n\nage100 &lt;- filter(starwars, birth_year &gt; 100)\nhead(age100)\n\n# A tibble: 5 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 C-3PO        167    75 &lt;NA&gt;       gold       yellow           112 none  mascu…\n2 Chewbacca    228   112 brown      unknown    blue             200 male  mascu…\n3 Jabba De…    175  1358 &lt;NA&gt;       green-tan… orange           600 herm… mascu…\n4 Yoda          66    17 white      green      brown            896 male  mascu…\n5 Dooku        193    80 white      fair       brown            102 male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\n\nYou can see that there are now only 5 rows in the data frame and the distribution of the birth_year values is.\n\nsummary(age100$birth_year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    102     112     200     382     600     896 \n\n\nWe can also filter on multiple conditions: and requires both conditions to be true, whereas or requires only one to be true. This time letʻs choose birth_year &lt; 100 and homeworld == \"Tatooine:\n\nage_tat &lt;- filter(starwars, birth_year &lt; 100 & homeworld == \"Tatooine\")\nselect(age_tat, name, height, mass, birth_year, sex)\n\n# A tibble: 8 × 5\n  name               height  mass birth_year sex   \n  &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; \n1 Luke Skywalker        172    77       19   male  \n2 Darth Vader           202   136       41.9 male  \n3 Owen Lars             178   120       52   male  \n4 Beru Whitesun Lars    165    75       47   female\n5 Biggs Darklighter     183    84       24   male  \n6 Anakin Skywalker      188    84       41.9 male  \n7 Shmi Skywalker        163    NA       72   female\n8 Cliegg Lars           183    NA       82   male  \n\n\nOther logical operators you should be aware of include:\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\n\n\n\n==\nEquals\nhomeworld == Tatooine\n\n\n!=\nDoes not equal\nhomeworld != Tatooine\n\n\n&gt;\nGreater than\nheight &gt; 170.0\n\n\n&gt;=\nGreater than or equal to\nheight &gt;= 170.0\n\n\n&lt;\nLess than\nheight &lt; 170.0\n\n\n&lt;=\nLess than or equal to\nheight &lt;= 170.0\n\n\n%in%\nIncluded in\nhomeworld %in% c(\"Tatooine\", \"Naboo\")\n\n\nis.na()\nIs a missing value\nis.na(mass)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are ever unsure of how to write a logical statement, but know how to write its opposite, you can use the ! operator to negate the whole statement.\nA common use of this is to identify observations with non-missing data (e.g., !(is.na(homweworld))).\n\n\nSorting data with arrange()\n\narrange() is like the sort function in a spreadsheet, or order() in base R. arrange() reorders rows of a data frame according to one of the columns. Think of this as sorting your rows on the value of a column.\nHere we can order the rows of the data frame by birth_year, so that the first row is the earliest (oldest) observation and the last row is the latest (most recent) observation.\n\nstarwars &lt;- arrange(starwars, birth_year)\n\nWe can now check the first few rows\n\nhead(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name                  birth_year\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Wicket Systri Warrick          8\n2 IG-88                         15\n3 Luke Skywalker                19\n\n\nand the last few rows.\n\ntail(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name           birth_year\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Poe Dameron            NA\n2 BB8                    NA\n3 Captain Phasma         NA\n\n\nColumns can be arranged in descending order using the helper function desc().\n\nstarwars &lt;- arrange(starwars, desc(birth_year))\n\nLooking at the first three and last three rows shows the dates in descending order.\n\nhead(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name                  birth_year\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Yoda                         896\n2 Jabba Desilijic Tiure        600\n3 Chewbacca                    200\n\ntail(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name           birth_year\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Poe Dameron            NA\n2 BB8                    NA\n3 Captain Phasma         NA\n\n\nRearranging columns with relocate()\n\nMoving a column to a new location is done by specifying the column names, and indicating where they go with the .before= or .after= arguments specifing a location (another column).\nrelocate(.data, ..., .before = NULL, .after = NULL)\nRenaming columns with rename()\n\nRenaming a variable in a data frame in R is accomplished using the names() function. The rename() function is designed to make this process easier.\nHere you can see the names of the first six variables in the starwars data frame.\n\nhead(starwars[, 1:6], 3)\n\n# A tibble: 3 × 6\n  name                  height  mass hair_color skin_color       eye_color\n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;    \n1 Yoda                      66    17 white      green            brown    \n2 Jabba Desilijic Tiure    175  1358 &lt;NA&gt;       green-tan, brown orange   \n3 Chewbacca                228   112 brown      unknown          blue     \n\n\nSuppose we wanted to drop the _color. The syntax is newname = oldname:\n\nstarwars &lt;- rename(starwars, hair = hair_color, skin = skin_color, eye = eye_color)\nhead(starwars[, 1:6], 3)\n\n# A tibble: 3 × 6\n  name                  height  mass hair  skin             eye   \n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt; \n1 Yoda                      66    17 white green            brown \n2 Jabba Desilijic Tiure    175  1358 &lt;NA&gt;  green-tan, brown orange\n3 Chewbacca                228   112 brown unknown          blue  \n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you do the equivalent in base R without dplyr?\n\n\nAdding columns with mutate()\n\nThe mutate() function computes transformations of variables in a data frame.\n\n\nArtwork by Allison Horst on mutate() function\n\n[Source: Artwork by Allison Horst]\nFor example, we may want to adjust height for mass:\n\nstarwars &lt;- mutate(starwars, heightsize = height / mass )\nhead(starwars)\n\n# A tibble: 6 × 15\n  name  height  mass hair  skin  eye   birth_year sex   gender homeworld species\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  \n1 Yoda      66    17 white green brown        896 male  mascu… &lt;NA&gt;      Yoda's…\n2 Jabb…    175  1358 &lt;NA&gt;  gree… oran…        600 herm… mascu… Nal Hutta Hutt   \n3 Chew…    228   112 brown unkn… blue         200 male  mascu… Kashyyyk  Wookiee\n4 C-3PO    167    75 &lt;NA&gt;  gold  yell…        112 none  mascu… Tatooine  Droid  \n5 Dooku    193    80 white fair  brown        102 male  mascu… Serenno   Human  \n6 Qui-…    193    89 brown fair  blue          92 male  mascu… &lt;NA&gt;      Human  \n# ℹ 4 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;,\n#   heightsize &lt;dbl&gt;\n\n\nThere is also the related transmute() function, which mutate()s and keeps only the transformed variables. Therefore, the result is only two columns in the transmuted data frame.\nPerform functions on groups using group_by()\n\nThe group_by() function is used to indicate groups within the data.\nFor example, what is the average height by homeworld?\nIn conjunction with the group_by() function, we often use the summarize() function.\n\n\n\n\n\n\nNote\n\n\n\nThe general operation here is a combination of\n\nSplitting a data frame by group defined by a variable or group of variables (group_by())\n\nsummarize() across those subsets\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nWe can create a separate data frame that splits the original data frame by homeworld.\n\nworlds &lt;- group_by(starwars, homeworld)\n\nCompute summary statistics by planet (just showing mean and median here, almost any summary stat is available):\n\nsummarize(worlds, height = mean(height, na.rm = TRUE), \n          maxheight = max(height, na.rm = TRUE),\n          mass = median(mass, na.rm = TRUE))\n\n# A tibble: 49 × 4\n   homeworld      height maxheight  mass\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Alderaan         176.      176.  64  \n 2 Aleen Minor       79        79   15  \n 3 Bespin           175       175   79  \n 4 Bestine IV       180       180  110  \n 5 Cato Neimoidia   191       191   90  \n 6 Cerea            198       198   82  \n 7 Champala         196       196   NA  \n 8 Chandrila        150       150   NA  \n 9 Concord Dawn     183       183   79  \n10 Corellia         175       175   78.5\n# ℹ 39 more rows\n\n\n\n\nsummarize() returns a data frame with homeworld as the first column, followed by the requested summary statistics. This is similar to the base R function aggregate().\n\n\n\n\n\n\nMore complicated example\n\n\n\nIn a slightly more complicated example, we might want to know what are the average masses within quintiles of height:\nFirst, we can create a categorical variable of height5 divided into quintiles\n\nqq &lt;- quantile(starwars$height, seq(0, 1, 0.2), na.rm = TRUE)\nstarwars &lt;- mutate(starwars, height.quint = cut(height, qq))\n\nNow we can group the data frame by the height.quint variable.\n\nquint &lt;- group_by(starwars, height.quint)\n\nFinally, we can compute the mean of mass within quintiles of height.\n\nsummarize(quint, mquint = mean(mass, na.rm = TRUE))\n\n# A tibble: 6 × 2\n  height.quint mquint\n  &lt;fct&gt;         &lt;dbl&gt;\n1 (66,165]       44.1\n2 (165,177]     178. \n3 (177,183]      79.3\n4 (183,193]      77.2\n5 (193,264]     106. \n6 &lt;NA&gt;           17  \n\n\n\n\nOddly enough there is a maximum mass in the second height quintile of Starwars characters. The biologist in me thinks maybe outliers?\nPiping multiple functions using %&gt;%\n\nThe pipe operator %&gt;% is very handy for stringing together multiple dplyr functions in a sequence of operations. It comes from the magritter package.\nSource:\nIn base R, there are two styles of applying multiple functions. The first is the resave the object after each operation.\nThe second is to nest functions, with the first at the deepest level (the heart of the onion), then working our way out:\n\nthird(second(first(x)))\n\nThe %&gt;% operator allows you to string operations in a left-to-right fashion, where the output of one flows into the next, i.e.:\n\nfirst(x) %&gt;% second %&gt;% third\n\n\n\n\n\n\n\nExample\n\n\n\nTake the example that we just did in the last section.\nThat can be done with the following sequence:\n\nstarwars %&gt;% \n  group_by(homeworld) %&gt;% \n  summarize(height = mean(height, na.rm = TRUE), \n          maxheight = max(height, na.rm = TRUE),\n          mass = median(mass, na.rm = TRUE))\n\n# A tibble: 49 × 4\n   homeworld      height maxheight  mass\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Alderaan         176.      176.  64  \n 2 Aleen Minor       79        79   15  \n 3 Bespin           175       175   79  \n 4 Bestine IV       180       180  110  \n 5 Cato Neimoidia   191       191   90  \n 6 Cerea            198       198   82  \n 7 Champala         196       196   NA  \n 8 Chandrila        150       150   NA  \n 9 Concord Dawn     183       183   79  \n10 Corellia         175       175   78.5\n# ℹ 39 more rows\n\n\n\n\nData masking\nNotice that we did not have to specify the dataframe. This is because dplyr functions are built on a data masking syntax. From the dplyr data-masking help page:\n\nData masking allows you to refer to variables in the “current” data frame (usually supplied in the .data argument), without any other prefix. It’s what allows you to type (e.g.) filter(diamonds, x == 0 & y == 0 & z == 0) instead of diamonds[diamonds$x == 0 & diamonds$y == 0 & diamonds$z == 0, ]\n\nWhen you look at the help page for ?mutate for example, you will see a function definition like so:\n\nmutate(.data, ...)\n\nNote the .data, Which means that the data can be supplied as usual, or it can be inherited from the “current” data frame which is passed to it via a pipe.\nSample rows of data with slice_*()\n\nThe slice_sample() function will randomly sample rows of data.\nThe number of rows to show is specified by the n argument.\n\nThis can be useful if you do not want to print the entire tibble, but you want to get a greater sense of the variation.\n\n\n\n\n\n\n\nExample\n\n\n\n\nslice_sample(starwars, n = 10)\n\n# A tibble: 10 × 16\n   name         height  mass hair  skin  eye   birth_year sex   gender homeworld\n   &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 Eeth Koth       171  NA   black brown brown       NA   male  mascu… Iridonia \n 2 Obi-Wan Ken…    182  77   aubu… fair  blue…       57   male  mascu… Stewjon  \n 3 Taun We         213  NA   none  grey  black       NA   fema… femin… Kamino   \n 4 Greedo          173  74   &lt;NA&gt;  green black       44   male  mascu… Rodia    \n 5 Leia Organa     150  49   brown light brown       19   fema… femin… Alderaan \n 6 Rey              NA  NA   brown light hazel       NA   fema… femin… &lt;NA&gt;     \n 7 Boba Fett       183  78.2 black fair  brown       31.5 male  mascu… Kamino   \n 8 R5-D4            97  32   &lt;NA&gt;  whit… red         NA   none  mascu… Tatooine \n 9 Bib Fortuna     180  NA   none  pale  pink        NA   male  mascu… Ryloth   \n10 Ric Olié        183  NA   brown fair  blue        NA   male  mascu… Naboo    \n# ℹ 6 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n#   starships &lt;list&gt;, heightsize &lt;dbl&gt;, height.quint &lt;fct&gt;\n\n\n\n\nYou can also use slice_head() or slice_tail() to take a look at the top rows or bottom rows of your tibble. Again the number of rows can be specified with the n argument.\nThis will show the first 5 rows.\n\nslice_head(starwars, n = 5)\n\n# A tibble: 5 × 16\n  name  height  mass hair  skin  eye   birth_year sex   gender homeworld species\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  \n1 Yoda      66    17 white green brown        896 male  mascu… &lt;NA&gt;      Yoda's…\n2 Jabb…    175  1358 &lt;NA&gt;  gree… oran…        600 herm… mascu… Nal Hutta Hutt   \n3 Chew…    228   112 brown unkn… blue         200 male  mascu… Kashyyyk  Wookiee\n4 C-3PO    167    75 &lt;NA&gt;  gold  yell…        112 none  mascu… Tatooine  Droid  \n5 Dooku    193    80 white fair  brown        102 male  mascu… Serenno   Human  \n# ℹ 5 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;,\n#   heightsize &lt;dbl&gt;, height.quint &lt;fct&gt;\n\n\nThis will show the last 5 rows.\n\nslice_tail(starwars, n = 5)\n\n# A tibble: 5 × 16\n  name  height  mass hair  skin  eye   birth_year sex   gender homeworld species\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  \n1 Finn      NA    NA black dark  dark          NA male  mascu… &lt;NA&gt;      Human  \n2 Rey       NA    NA brown light hazel         NA fema… femin… &lt;NA&gt;      Human  \n3 Poe …     NA    NA brown light brown         NA male  mascu… &lt;NA&gt;      Human  \n4 BB8       NA    NA none  none  black         NA none  mascu… &lt;NA&gt;      Droid  \n5 Capt…     NA    NA none  none  unkn…         NA fema… femin… &lt;NA&gt;      Human  \n# ℹ 5 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;,\n#   heightsize &lt;dbl&gt;, height.quint &lt;fct&gt;\n\n\nSummary\nThe dplyr package provides an alternative syntax for manipulating data frames. In particular, we can often conduct the beginnings of an exploratory analysis with the powerful combination of group_by() and summarize().\nOnce you learn the dplyr grammar there are a few additional benefits\n\ndplyr can work with other data frame “back ends” such as SQL databases. There is an SQL interface for relational databases via the DBI package\ndplyr can be integrated with the data.table package for large fast tables\nMany people like the piping syntax for readability and clarity\nPost-lecture materials\nFinal Questions\n\n\n\n\n\n\nQuestions\n\n\n\n\nHow can you tell if an object is a tibble?\nUsing the trees dataset in base R (this dataset stores the girth, height, and volume for Black Cherry Trees) and using the pipe operator:\n\nconvert the data.frame to a tibble.\nfilter for rows with a tree height of greater than 70, and\norder rows by Volume (smallest to largest).\n\n\n\n\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio"
  },
  {
    "objectID": "posts/2025-01-16-your-computer-filesystems/index.html",
    "href": "posts/2025-01-16-your-computer-filesystems/index.html",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "",
    "text": "Read ahead\n\n\n\nFor future lectures, Iʻll give you some reading or podcasts to prepare\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://academind.com/tutorials/terminal-zsh-basics"
  },
  {
    "objectID": "posts/2025-01-16-your-computer-filesystems/index.html#the-kernel",
    "href": "posts/2025-01-16-your-computer-filesystems/index.html#the-kernel",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The kernel",
    "text": "The kernel\nThe kernel is the part of your computerʻs operating system that loads first once you start up. It is kind of like your computerʻs autonomic nervous system. It recognizes all of the physical hardware attached to it, enables communication between components (and device drivers), and monitors maintenance functions like turning on the fan when it gets hot, manages virtual memory, gives warnings when the hard drive gets full, manages multitasking, and manages security and file permissions. In the mac this is the XNU kernel (“X is not UNIX”), in modern Windows machines it is the Windows NT kernel.\n\n\n\n\n\n\n\n\n[Source: Map of MacOS: the heart of everything is called Darwin; and within it, we have separate system utilities (the shell) and the XNU kernel, which is composed in parts by the Mach kernel and by the BSD kernel.]"
  },
  {
    "objectID": "posts/2025-01-16-your-computer-filesystems/index.html#the-shell",
    "href": "posts/2025-01-16-your-computer-filesystems/index.html#the-shell",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The shell",
    "text": "The shell\nThe shell is another key part of the core operating system (note in the diagram above it is part of the System Utilities, and the partner of the kernel). The shell is a software (an app) that allows humans to control the computer. You are already familiar with the GUI interface, or the Graphical User Interface. It is important that you are comfortable using the Command Line Interface as well.\n\n\n\n\n\n\nThere are many reasons to be proficient in the shell:\n\n\n\n\nData analysis increasingly uses many files. The shell provides a simple but very powerful means to do all kinds of operations on files: move, delete, organize, combine, rename, etc.\nUsing the shell encourages you to understand your computerʻs filesystem, and helps you to more precisely control input and output to any place along your file paths.\nShell operations are fast.\nYou can use wildcards to control matching or excluding many files.\nThe shell can be used to execute (run) software.\nThe shell is probably the oldest app, so it is very stable with lasting power.\nIt is part of the OS, so when your apps fail or you are having some issues, you would turn to the shell to kill troublesome processes (programs) or diagnose and fix the issues.\n\n\n\nMacs use the same terminal utilities as UNIX/Linux systems. On the Mac, the command line interface app is called Terminal, which you will find in your Application folder, in the Utilities subfolder (here is a screentshot of our GUI Interface).\n\n\n\n\n\n\n\n\nOn a PC if you installed Git For Windows, you can use Git-Bash (a UNIX emulator) and follow the UNIX instructions (this would be my personal preference). Otherwise you use the Command Prompt also known as the Windows Command Processor or CMD. If you used a pre-Windows machine, you would be familiar with MS-DOS. To open CMD:\n\nOpen the Start Menu and type “command prompt” or\nPress Win + R and type “cmd” in the run box or\nPress Win + X and select Command Prompt from the menu.\n\nNote: you may see Windows PowerShell or Windows Terminal instead, these are similar apps."
  },
  {
    "objectID": "posts/2025-01-16-your-computer-filesystems/index.html#the-dots",
    "href": "posts/2025-01-16-your-computer-filesystems/index.html#the-dots",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The dots",
    "text": "The dots\n\n“.” is the current working directory (where you are currently)\n“..” is the directory one level up\n“./foldername” will take you to the folder one level down, for example “./Data”\n\nYou can use these paths to change directories using cd or to list ls the contents of the directories or to make new directories using mkdir\nls .\nls ./Data\nmkdir ./Data/A\nMake multiple directories:\nmkdir ./Data/B ./Data/C\nList the files one level up or two levels up:\nls ..  # for PC use dir ..\nls ../..\nUp one level, and over to another directory:\nls ../AnotherDirectory\nYou can wander anywhere along your computerʻs file directory! Just add more steps to the path."
  },
  {
    "objectID": "posts/2025-02-25-ggplot2/index.html",
    "href": "posts/2025-02-25-ggplot2/index.html",
    "title": "The ggplot2 package",
    "section": "",
    "text": "For more details see\n\n\n\n\nWonderful Window shopping in the R graph gallery (with code): https://r-graph-gallery.com\n\nThe “grammar of graphics” explained in Hadley Wickamʻs article: http://vita.had.co.nz/papers/layered-grammar.pdf\n\nVery gentle intro for beginners: https://posit.cloud/learn/primers/3\n\nHadley Wickamʻs overview: https://r4ds.had.co.nz/data-visualisation\n\nCedric Schererʻs Step-by-step tutorial: https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/\n\nFor in-depth reading, Hadley Wickamʻs ggplot2 book: https://ggplot2-book.org"
  },
  {
    "objectID": "posts/2025-02-25-ggplot2/index.html#basic-components-of-a-ggplot2-plot",
    "href": "posts/2025-02-25-ggplot2/index.html#basic-components-of-a-ggplot2-plot",
    "title": "The ggplot2 package",
    "section": "Basic components of a ggplot2 plot",
    "text": "Basic components of a ggplot2 plot\n\n\n\n\n\n\nKey components\n\n\n\nA ggplot2 plot consists of a number of key components.\n\n\nData: In the form of a dataframe or tibble, containing all of the data that will be displayed on the plot.\n\nGeometry: The geometry or geoms define the style of the plot such as scatterplot, barplot, histogram, violin plots, smooth densities, qqplot, boxplot, and others.\n\nAesthetic mapping: Aesthetic mappings describe how data are mapped to color, size, shape, location, or to legend elements. How we define the mapping depends on what geometry we are using.\n\nNearly all plots drawn with ggplot2 will have the above compoents. In addition you may want to have specify additional elements:\n\nFacets: When used, facets describe how panel plots based on partions of the data should be drawn.\nStatistical Transformations: Or stats are transformations of the data such as log-transformation, binning, quantiles, smoothing.\nScales: Scales are used to indicate which factors are associated with the levels of the aesthetic mapping. Use manual scales to specify each level.\nCoordinate System: ggplot2 will use a default coordinate system drawn from the data, but you can customize the coordinate system in which the locations of the geoms will be drawn\n\n\n\nPlots are built up in layers, with the typical ordering being\n\nPlot the data\nOverlay a summary that reveals the relationship\nAdd metadata and annotation"
  },
  {
    "objectID": "posts/2025-02-25-ggplot2/index.html#legend",
    "href": "posts/2025-02-25-ggplot2/index.html#legend",
    "title": "The ggplot2 package",
    "section": "Legend",
    "text": "Legend\nFor example, we can make changes to the legend title via the scale_color_discrete function():\n\np + geom_point(aes(col=Species), size = 3) + \n  scale_color_discrete(name = \"Iris Varieties\")"
  },
  {
    "objectID": "posts/2025-02-25-ggplot2/index.html#no-legend",
    "href": "posts/2025-02-25-ggplot2/index.html#no-legend",
    "title": "The ggplot2 package",
    "section": "No legend",
    "text": "No legend\nggplot2 automatically adds a legend that maps color to species. To remove the legend we set the geom_point() argument show.legend = FALSE.\n\np + geom_point(aes(col=Species), size = 3, show.legend=FALSE)"
  },
  {
    "objectID": "posts/2025-02-25-ggplot2/index.html#themes",
    "href": "posts/2025-02-25-ggplot2/index.html#themes",
    "title": "The ggplot2 package",
    "section": "Themes",
    "text": "Themes\nThe default theme for ggplot2 uses the gray background with white grid lines.\nIf you don’t like this, you can use the black and white theme by using the theme_bw() function.\nThe theme_bw() function also allows you to set the typeface for the plot, in case you don’t want the default Helvetica. Here we change the typeface to Times.\n\n\n\n\n\n\nNote\n\n\n\nFor things that only make sense globally, use theme(), i.e. theme(legend.position = \"none\"). Two standard appearance themes are included\n\n\ntheme_gray(): The default theme (gray background)\n\ntheme_bw(): More stark/plain\n\n\n\n\np + \n  geom_point(aes(color = Species)) + \n  theme_bw(base_family = \"Times\")\n\n\n\nModifying the theme for a plot"
  },
  {
    "objectID": "posts/2025-02-25-ggplot2/index.html#ggthemes",
    "href": "posts/2025-02-25-ggplot2/index.html#ggthemes",
    "title": "The ggplot2 package",
    "section": "ggthemes",
    "text": "ggthemes\nThe style of a ggplot2 graph can be changed using the theme functions. Several themes are included as part of the ggplot2 package.\nMany other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this:\n\nlibrary(ggthemes)\n\n\nAttaching package: 'ggthemes'\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\np + geom_point(aes(col=Species, size = 3, alpha = 1/2)) +\n  scale_color_manual(values=cols) + \n  theme_economist()\n\n\n\n\n\n\n\nYou can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead.\nggrepel\nThe final difference has to do with the position of the labels. In our plot, some of the labels fall on top of each other. The package ggrepel includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change geom_text with geom_text_repel.\n\nlibrary(ggrepel)\np + geom_point(aes(col=Species, size = 3, alpha = 1/2)) +\n  scale_color_manual(values=cols) + \n  theme_economist() +\n  geom_text_repel(aes(Petal.Length, Petal.Width, label = id))\n\nWarning: ggrepel: 90 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "posts/2025-02-18-tidying-exploring-data/index.html",
    "href": "posts/2025-02-18-tidying-exploring-data/index.html",
    "title": "Tidying and Exploring Data",
    "section": "",
    "text": "Watch\n\n\n\nBefore class, you can prepare by watching:\n\n\n\n\n\n\n\n✏️"
  },
  {
    "objectID": "posts/2025-02-18-tidying-exploring-data/index.html#vectors",
    "href": "posts/2025-02-18-tidying-exploring-data/index.html#vectors",
    "title": "Tidying and Exploring Data",
    "section": "Vectors",
    "text": "Vectors\nThe index of a vector is it’s number in the array. Each and every element in any data object has at least one index (if vector, it is one dimensional so it is its position along the vector, if a matrix or data frame, which are two-dimensional, it’s the row and column number, etc.)\nLet’s create a vector:\n\nxx &lt;- c(1, 5, 2, 3, 5)\nxx\n\n[1] 1 5 2 3 5\n\n\nAccess specific values of xx by number:\n\nxx[1]\n\n[1] 1\n\nxx[3]\n\n[1] 2\n\n\nYou can use a function to generate an index. Get the last element (without knowing how many there are) by:\n\nxx[length(xx)]\n\n[1] 5\n\n\nRetrieve multiple elements of xx by using a vector as an argument:\n\nxx[c(1, 3, 4)]\n\n[1] 1 2 3\n\nxx[1:3]\n\n[1] 1 5 2\n\nxx[c(1, length(xx))]  # first and last\n\n[1] 1 5\n\n\nExclude elements by using a negative index:\n\nxx\n\n[1] 1 5 2 3 5\n\nxx[-1]  # exclude first\n\n[1] 5 2 3 5\n\nxx[-2] # exclude second\n\n[1] 1 2 3 5\n\nxx[-(1:3)] # exclude first through third\n\n[1] 3 5\n\nxx[-c(2, 4)] # exclude second and fourth, etc. \n\n[1] 1 2 5\n\n\nUse a logical vector:\n\nxx[ c( T, F, T, F, T) ]  # T is the same as TRUE\n\n[1] 1 2 5\n\n\n\nxx &gt; 2\n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\nxx[ xx &gt; 2 ]\n\n[1] 5 3 5\n\nxx &gt; 2 & xx &lt; 5\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\nxx[ xx&gt;2 & xx&lt;5]\n\n[1] 3\n\n\nSubsetting (picking particular observations out of an R object) is something that you will have to do all the time. It’s worth the time to understand it clearly.\n\nsubset_xx &lt;-  xx[ xx &gt; 2 ]\nsubset_xx2 &lt;- subset(xx, xx&gt;2)  # using subset function\nsubset_xx == subset_xx2   # check if the same\n\n[1] TRUE TRUE TRUE\n\n\nThe subset function is just another way of subsetting by index, just in function form with arguments. It can be more clear to use for dataframes, but it is really a matter of personal preference as you develop your style. Whichever way you go, it is important to be aware of the different ways to achieve the same goals."
  },
  {
    "objectID": "posts/2025-02-18-tidying-exploring-data/index.html#matrices-and-dataframes",
    "href": "posts/2025-02-18-tidying-exploring-data/index.html#matrices-and-dataframes",
    "title": "Tidying and Exploring Data",
    "section": "Matrices and Dataframes",
    "text": "Matrices and Dataframes\nMatrices and dataframes are both rectangular having two dimensions, and are handled very similarly for indexing and subsetting.\nLet’s work with a dataframe that is provided with the geiger package called geospiza. It is a list with a tree and a dataframe. The dataframe contains five morphological measurements for 13 species. First, let’s clear the workspace (or clear and start a new R session):\n\ninstall.packages(\"geiger\")  # if you need to install geiger\n\nGet the built-in dataset this way:\n\nrm(list=ls())\nlibrary(geiger)\n\nLoading required package: ape\n\n\nLoading required package: phytools\n\n\nLoading required package: maps\n\ndata(geospiza)   # load the dataset into the workspace\nls()               # list the objects in the workspace\n\n[1] \"geospiza\"\n\n\nLet’s find out some basic information about this object:\n\nclass(geospiza)\n\n[1] \"list\"\n\nattributes(geospiza)\n\n$names\n[1] \"geospiza.tree\" \"geospiza.data\" \"phy\"           \"dat\"          \n\nstr(geospiza)\n\nList of 4\n $ geospiza.tree:List of 4\n  ..$ edge       : num [1:26, 1:2] 15 16 17 18 19 20 21 22 23 24 ...\n  ..$ edge.length: num [1:26] 0.2974 0.0492 0.0686 0.134 0.1035 ...\n  ..$ Nnode      : int 13\n  ..$ tip.label  : chr [1:14] \"fuliginosa\" \"fortis\" \"magnirostris\" \"conirostris\" ...\n  ..- attr(*, \"class\")= chr \"phylo\"\n $ geospiza.data: num [1:13, 1:5] 4.4 4.35 4.22 4.26 4.24 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:13] \"magnirostris\" \"conirostris\" \"difficilis\" \"scandens\" ...\n  .. ..$ : chr [1:5] \"wingL\" \"tarsusL\" \"culmenL\" \"beakD\" ...\n $ phy          :List of 4\n  ..$ edge       : num [1:26, 1:2] 15 16 17 18 19 20 21 22 23 24 ...\n  ..$ edge.length: num [1:26] 0.2974 0.0492 0.0686 0.134 0.1035 ...\n  ..$ Nnode      : int 13\n  ..$ tip.label  : chr [1:14] \"fuliginosa\" \"fortis\" \"magnirostris\" \"conirostris\" ...\n  ..- attr(*, \"class\")= chr \"phylo\"\n $ dat          : num [1:13, 1:5] 4.4 4.35 4.22 4.26 4.24 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:13] \"magnirostris\" \"conirostris\" \"difficilis\" \"scandens\" ...\n  .. ..$ : chr [1:5] \"wingL\" \"tarsusL\" \"culmenL\" \"beakD\" ...\n\n\nIt is a list with four elements. Here we want the data\n\ngeo &lt;- geospiza$geospiza.data\ndim(geo)\n\n[1] 13  5\n\n\nIts a rectangle of data, but let’s see what type of object it is:\n\nclass(geo)\n\n[1] \"matrix\" \"array\" \n\n\nIt is a matrix, but we want to work with a dataframe, so let’s coerce it:\n\ngeo &lt;- as.data.frame(geo)\n\nIf we want to know all the attributes of geo:\n\nattributes(geo)\n\n$names\n[1] \"wingL\"   \"tarsusL\" \"culmenL\" \"beakD\"   \"gonysW\" \n\n$class\n[1] \"data.frame\"\n\n$row.names\n [1] \"magnirostris\" \"conirostris\"  \"difficilis\"   \"scandens\"     \"fortis\"      \n [6] \"fuliginosa\"   \"pallida\"      \"fusca\"        \"parvulus\"     \"pauper\"      \n[11] \"Pinaroloxias\" \"Platyspiza\"   \"psittacula\"  \n\n\nIt is a dataframe with 13 rows and 5 columns. We see that it has a “names” attribute, which refers to column names in a dataframe. Typically, the columns of a dataframe are the variables in the dataset. It also has “rownames” which contains the species names (so it does not have a separate column for species names).\nDataframes have two dimensions which we can use to index with: dataframe[row, column].\n\ngeo     # the entire object, same as geo[] or geo[,]\ngeo[c(1, 3), ]   # select the first and third rows, all columns\ngeo[, 3:5]   # all rows, third through fifth columns\ngeo[1, 5]  # first row, fifth column (a single number)\ngeo[1:2, c(3, 1)]  # first and second row, third and first column (2x2 matrix)\ngeo[-c(1:3, 10:13), ]  # everything but the first three and last three rows\ngeo[ 1:3, 5:1]  # first three species, but variables in reverse order\n\nTo prove to ourselves that we can access matrices in the same way, let’s coerce geo to be a matrix:\n\ngeom &lt;- as.matrix( geo ) \nclass(geom)\n\n[1] \"matrix\" \"array\" \n\nclass(geo)\n\n[1] \"data.frame\"\n\ngeo[1,5]  # try a few more from the choices above to test\n\n[1] 2.675983\n\n\nSince geo and geom have row and column names, we can access by name (show that this works for geom too):\n\ngeo[\"pauper\", \"wingL\"]  # row pauper, column wingL\n\n[1] 4.2325\n\ngeo[\"pauper\", ]  # row pauper, all columns \n\n        wingL tarsusL culmenL  beakD gonysW\npauper 4.2325  3.0359   2.187 2.0734 1.9621\n\n\nWe can also use the names (or rownames) attribute if we are lazy. Suppose we wanted all the species which began with “pa”. we could find which position they hold in the dataframe by looking at the rownames, saving them to a vector, and then indexing by them:\n\nsp &lt;- rownames(geo)\nsp                            # a vector of the species names\n\n [1] \"magnirostris\" \"conirostris\"  \"difficilis\"   \"scandens\"     \"fortis\"      \n [6] \"fuliginosa\"   \"pallida\"      \"fusca\"        \"parvulus\"     \"pauper\"      \n[11] \"Pinaroloxias\" \"Platyspiza\"   \"psittacula\"  \n\nsp[c(7,8,10)]     # the ones we want are #7,8, and 10\n\n[1] \"pallida\" \"fusca\"   \"pauper\" \n\ngeo[ sp[c(7,8,10)], ]  # rows 7,8 and 10, same as geo[c(7, 8, 10)]\n\n           wingL  tarsusL  culmenL    beakD   gonysW\npallida 4.265425 3.089450 2.430250 2.016350 1.949125\nfusca   3.975393 2.936536 2.051843 1.191264 1.401186\npauper  4.232500 3.035900 2.187000 2.073400 1.962100\n\n\nOne difference between dataframes and matrices is that Indexing a data frame by a single vector (meaning, no comma separating) selects an entire column. This can be done by name or by number:\n\ngeo[3]   # third column\ngeo[\"culmenL\"]  # same\ngeo[c(3,5)]  # third and fifth column\ngeo[c(\"culmenL\", \"gonysW\")]  # same\n\nProve to yourself that selecting by a single index has a different behavior for matrices (and sometimes produces an error.\n\n\n\n\n\n\nWhy?\n\n\n\n\nBecause internally, a dataframe is actually a list of vectors. Thus a single name or number refers to the column, rather than a coordinate in a cartesian-coordinate-like system.\nHowever, a matrix is actually a vector with breaks in it. So a single number refers to a position along the single vector.\nA single name could work, but only if the individual elements of the matrix have names (like naming the individual elements of a vector).\n\n\n\nAnother difference is that dataframes (and lists below) can be accessed by the $ operator. It means indicates a column within a dataframe, so dataframe$column. This is another way to select by column:\n\ngeo$culmenL\n\n [1] 2.724667 2.654400 2.277183 2.621789 2.407025 2.094971 2.430250 2.051843\n [9] 1.974420 2.187000 2.311100 2.331471 2.259640\n\n\nAn equivalent way to index is by using the subset function. Some people prefer it because you have explicit parameters for what to select and which variables to include. See help page ?subset."
  },
  {
    "objectID": "posts/2025-02-18-tidying-exploring-data/index.html#lists",
    "href": "posts/2025-02-18-tidying-exploring-data/index.html#lists",
    "title": "Tidying and Exploring Data",
    "section": "Lists",
    "text": "Lists\nA list is a vector, except that whereas an ordinary vector has the same type of data (numeric, character, factor) in each slot, a list can have different types in different slots. They are sort of like expandable containers, flexibly accommodating any group of objects that the user wants to keep together.\nThey are accessed by numeric index or by name (if they are named), but they are accessed by double square brackets. Also, you can’t access multiple elements of lists by using vectors of indices:\n\nmylist &lt;- list( vec = 2*1:10, mat = matrix(1:10, nrow=2), cvec = c(\"frogs\", \"birds\"))\nmylist\n\n$vec\n [1]  2  4  6  8 10 12 14 16 18 20\n\n$mat\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n$cvec\n[1] \"frogs\" \"birds\"\n\nmylist[[2]]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\nmylist[[\"vec\"]]\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n# mylist[[1:3]]  # gives an error if you uncomment it\nmylist$cvec\n\n[1] \"frogs\" \"birds\""
  },
  {
    "objectID": "posts/2025-02-18-tidying-exploring-data/index.html#multiple-arguments---breaking-ties",
    "href": "posts/2025-02-18-tidying-exploring-data/index.html#multiple-arguments---breaking-ties",
    "title": "Tidying and Exploring Data",
    "section": "Multiple arguments - breaking ties",
    "text": "Multiple arguments - breaking ties\nOrder can sort on multiple arguments, which means that you can use other columns to break ties. Let’s trim the species names to the first letter using the substring function, then sort using the first letter of the species name and breaking ties by tarsusL:\n\nsp &lt;- substring(rownames(geo), first=1, last=1)\noo &lt;- order(sp , geo$tarsusL) # order by first letter species, then tarsusL\ngeot &lt;- geo[oo,][\"tarsusL\"]   # ordered geo dataframe, take only the wingL column\ngeo &lt;- geo[oo,]\n\n\n\n\n\n\n\nNote:\n\n\n\n\nUsing geo[\"tarsusL\"] as a second index for order doesn’t work, because it is a one column dataframe, as opposed to geo$tarsus which is a vector.\nThe object must match sp, which is a vector. Check the dim and length of each.\nvectors only have length, dataframes have dimensions (dim=2 for numbers of rows, columns).\n\n\nCheck your objects! The shapes have to be of the right class even if they contain the same information. This is because functions are written for particular classes of objects (and sometimes the programmer didnʻt write one for your type of object).\nYou can always coerce your object to the right class using as.vector, as.dataframe, etc. and try again."
  },
  {
    "objectID": "posts/2025-02-18-tidying-exploring-data/index.html#missing-values",
    "href": "posts/2025-02-18-tidying-exploring-data/index.html#missing-values",
    "title": "Tidying and Exploring Data",
    "section": "Missing Values",
    "text": "Missing Values\nMissing values compared to anything else will return a missing value (so NA == NA returns NA, which is usually not what you want). You must test it with is.na function. You can also test multiple conditions with and (&) and or (|)\n\n!is.na(geo$gonysW) \n\n [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n[13]  TRUE\n\ngeo[!is.na(geo$gonysW) & geo$wingL &gt; 4, ]  # element by element \"and\"\n\n                wingL  tarsusL  culmenL    beakD   gonysW\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\nplatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\n\ngeo[!is.na(geo$gonysW) | geo$wingL &gt; 4, ]   # element by element \"or\"\n\n                wingL  tarsusL  culmenL    beakD   gonysW\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\ndifficilis   4.224067 2.898917 2.277183 2.011100       NA\nfuliginosa   4.132957 2.806514 2.094971       NA       NA\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\nparvulus     4.131600 2.973060       NA       NA       NA\npinaroloxias 4.188600 2.980200 2.311100       NA       NA\npauper       4.232500 3.035900 2.187000 2.073400       NA\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\npallida      4.265425 3.089450 2.430250 2.016350       NA\nplatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\n\n\nMatching works on strings also:\n\ngeo[rownames(geo) == \"pauper\",]   # same as   geo[\"pauper\", ]\ngeo[rownames(geo) &lt; \"pauper\",]\n\nThere are even better functions for strings, though. In the expression A %in% B, the %in% operator compares two vectors of strings, and tells us which elements of A are present in B.\n\nnewsp &lt;- c(\"clarkii\", \"pauper\", \"garmani\")\nnewsp[newsp  %in% rownames(geo)]     # which new species are in geo?\n\nWe can define the “without” operator:\n\n\"%w/o%\" &lt;- function(x, y) x[!x %in% y]\nnewsp  %w/o% rownames(geo)   # which new species are not in geo?"
  },
  {
    "objectID": "posts/2025-02-18-tidying-exploring-data/index.html#plot",
    "href": "posts/2025-02-18-tidying-exploring-data/index.html#plot",
    "title": "Tidying and Exploring Data",
    "section": "Plot()",
    "text": "Plot()\nFor a generic x-y plot use plot(). It will also start a graphical device.\nHere we are using the with() function to specify which dataframe to look into for our named variables. We could instead do penguins$body_mass_g, etc.\n\nwith(penguins, plot( body_mass_g, bill_length_mm))\n\n\n\n\n\n\n\nTo add points to an existing plot, use points()\n\nwith(penguins, plot( body_mass_g, bill_length_mm))\nwith(penguins[penguins$species==\"Adelie\",], points( body_mass_g, bill_length_mm, col=\"red\"))\n\n\n\n\n\n\n\nOne could fit linear models, for example, and use lines() to overlay the line on the plot."
  },
  {
    "objectID": "posts/2025-02-18-tidying-exploring-data/index.html#distribution-using-hist-and-density",
    "href": "posts/2025-02-18-tidying-exploring-data/index.html#distribution-using-hist-and-density",
    "title": "Tidying and Exploring Data",
    "section": "Distribution using hist() and density()",
    "text": "Distribution using hist() and density()\nTo see a histogram, use hist(). You can change the breaks and many other features by checking out the help page ?hist\n\nwith(penguins, hist( body_mass_g ))\n\n\n\n\n\n\n\nTo see a density plot use density() to create the density, then plot it.\n\ndens &lt;- with(penguins, density( body_mass_g, na.rm=T ))\nplot(dens)"
  },
  {
    "objectID": "posts/2025-03-11-multivariate/index.html",
    "href": "posts/2025-03-11-multivariate/index.html",
    "title": "A small tour of multivariate analysis",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to perform basic univariate statistics\nBe able to perform basic multivariate statistics\nBe able to relate questions to graphical representations of data"
  },
  {
    "objectID": "posts/2025-03-11-multivariate/index.html#nature-of-the-relationship-among-variables",
    "href": "posts/2025-03-11-multivariate/index.html#nature-of-the-relationship-among-variables",
    "title": "A small tour of multivariate analysis",
    "section": "Nature of the relationship among variables",
    "text": "Nature of the relationship among variables\nYou may also expect your data to follow a power law, in which case a log-transformation will make the data linear. For example, things that scale with body size tend to have the form:\n\\[\nY = aMass^b  \n\\] \\[\nlog(Y) = log(a) + b\\times log(Mass)\n\\]"
  },
  {
    "objectID": "posts/2025-03-11-multivariate/index.html#fitting-assumptions-of-parametric-statistics",
    "href": "posts/2025-03-11-multivariate/index.html#fitting-assumptions-of-parametric-statistics",
    "title": "A small tour of multivariate analysis",
    "section": "Fitting assumptions of parametric statistics",
    "text": "Fitting assumptions of parametric statistics\nIf you plan to do parametric statistics, for example many forms of regression, ANOVA, etc. one of the major assumptions is that the errors are normally distributed.\nThat is, the relationship follows the form:\n\\[\nY \\sim X + e\n\\]\nWhich is read as Y is proportional to X plus random error. Where e ~ N or the errors or deviations from this relationship follow a normal distribution. Note that this assumes that X is known without error.\nChecking for normality\nA convenient tool for checking the normality of continuous data is qqnorm() which plots the QQ quantiles of the data. If it is normally distributed, the points should fall on a straight line:\n\nqqnorm(iris$Sepal.Length)\nqqline(iris$Sepal.Length)\n\n\n\n\n\n\n\nOr the ggplot2 version:\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\nrequire(magrittr)  # for piping using %&gt;%\n\nLoading required package: magrittr\n\nggplot(iris, aes(sample=Sepal.Length)) + \n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\nThis data looks pretty good, except for some deviations along the edges. Most data will not ever be perfectly normal, you will get a sense of what is acceptable with more experience.\nHowever, we do know that this data contains three species – what happens if you were to look at the data by species?\n\niris %&gt;% \n    ggplot( aes(sample=Sepal.Length)) + \n   stat_qq( aes(col=Species)) +\n   stat_qq_line( aes(col=Species) )\n\n\n\n\n\n\n\nSkew\nDeviations from normality are not the end of the world, and often a little is tolerated. What can be more problematic is strong skew. For that you will really want to transform the data:\nSource\nRight skewed data is data with a long tail to the right (the positive side). Left skeweed data has along tail to the left. Here are a few methods. There are more\n\n\nSkew\nTransform\nCode\n\n\n\nstrong right\ncube root\nz = x^(1/3)\n\n\n\nsquare root\nz = x6(1/2)\n\n\n\nlog\nz = log(x)\n\n\n\n\nz = log10(x)\n\n\n\n\nz = log2(x)\n\n\nstrong left\nsquare\nz = x^2"
  },
  {
    "objectID": "posts/2025-03-11-multivariate/index.html#separating-size-and-shape",
    "href": "posts/2025-03-11-multivariate/index.html#separating-size-and-shape",
    "title": "A small tour of multivariate analysis",
    "section": "Separating Size and Shape",
    "text": "Separating Size and Shape\nWill you want to do an analysis of the data along with a size-corrected dataset? If shape variation is interesting for your data (i.e., do they differ in shape when we control for differences in size, or are they relatively larger or smaller?), then you may want to find some sort of size-adjustment. Popular methods include\n\nRegressing against a size variable, and using residuals\n\nPCA analysis excluding PC1 (PC1 is considered size),\n\nShear or Procrustes methods, and\n\nRatios with size.\nThere is a huge wealth of literature on size and how to analyze shape."
  },
  {
    "objectID": "posts/2025-03-11-multivariate/index.html#pca-loadings",
    "href": "posts/2025-03-11-multivariate/index.html#pca-loadings",
    "title": "A small tour of multivariate analysis",
    "section": "PCA Loadings",
    "text": "PCA Loadings\nSome things to look for in PC analysis: The loadings of the variables on the PC axes show how much each variable is correlated with that PC axis. The magnitude of the loading indicates how strong the correlation is, and the sign indicates the direction. The sign of the loading is only informative if variables load with different signs on the same PC axis. For example if variable A and B load positively with PC 2, and variable C loads negatively, this is often interpreted as varying along PC2 in an increasing direction indicating larger A and B but smaller C. In a morphological analysis, the first PC axis often loads positively and nearly equally on all variables, and is therefore considered to indicate size. PC1 also typically explains a large fraction of the variation.\nThe amount of variation each PC axis explains is called the proportion of variance explained. It is usually expressed as a percent or a fraction. It is not uncommon in morphological analysis for PC1 to explain 90% of the variation in the data.\nIt is important to note, however, that the amount of variation does not necessarily indicate it’s importance. Many ecological associations or functionally significant variation is reflected in shape variation, which as we said may be only 10% of the variation. However, this might be very functionally relevant. Size may vary a lot, but it might be whether or not you have very long legs relative to your size that tells us if you are a good runner. Long legs (in an absolute sense) may not make you a great runner if you are actually huge in size, so that relative to your body length, your legs are actually relatively short. So one thing to keep in mind is that you often will use only 3 axes, even though you have 10 or more variables. If you have managed to capture 90 or 95% of the variation with the first three variables (sometimes even more), you’re probably in great shape. It’s a tradeoff between keeping the analysis and interpretation manageable, and keeping all the variation in the data. Usually the minor axes have less than 1% of the variation, and are usually not interesting even if you were to keep them. Anyway, to conclude this paragraph, you may want to do a PC analysis on the data with size included, and then do a second analysis on the size-adjusted data (shape). Another strategy is to do a PC analysis on the data with size, and then leave out PC1 in downstream analyses of “shape”."
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html",
    "href": "posts/2025-01-21-intro-git-github/index.html",
    "title": "Introduction to git/GitHub",
    "section": "",
    "text": "Note 1: Helpful references for this lecture\n\n\n\n\n\nHappy Git with R from Jenny Bryan\n\nChapter on git and GitHub in dsbook from Rafael Irizarry\n\nGitHub introduction in module 1 from Andreas Handel\nEither before or after Tuesday’s class, please watch the podcast GitHub my Computer and me. You can stop at ~11:15:\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/\nhttps://andreashandel.github.io/MADAcourse/Tools_Github_Introduction.html"
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html#acknowledgements",
    "href": "posts/2025-01-21-intro-git-github/index.html#acknowledgements",
    "title": "Introduction to git/GitHub",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/\nhttps://andreashandel.github.io/MADAcourse/Tools_Github_Introduction.html"
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html#git",
    "href": "posts/2025-01-21-intro-git-github/index.html#git",
    "title": "Introduction to git/GitHub",
    "section": "git",
    "text": "git\nGit is software that implements what is called a version control system for a repository of files (also known as a repo). The main idea is that as you (and your collaborators) work on a project, the git software tracks, and records any changes made by anyone.\nGitHub is an online server and user interface that provides powerful tools for distribution of your repository, bug tracking, collaboration, and also allows you to create easy websites for each repository.\nGit and GitHub together provide an organized way to track your projects, and all of the tools you will need in this course are free.\n\n\n\n\nFigure 1. Whether working solo (A) or collaborating in a group (B) version tracking by naming files is a mess when you want to retrace the steps of the analysis (C). Git and GitHub track all changes and the complete branching tree of the project (D). The commit history is a powerful tool to retrace the development of the project or can be used to roll back to any prior version.\n\n\n\n[Source: Jenny Bryan]\nIt is very well suited for collaborative work, as it was developed by Linus Torvalds (in about 10 days of coding!) for collaborative software development of the Linux kernel pretty interesting interview with Torvalds. What it did really well was distributed control, and allowing everyone to have their own copy of the repository.\nGit/GitHub is now the dominant version control system with GitHub hosting over 200 million repositories worldwide! It is used very broadly for all kinds of repos now including data science projects, book projects, courses, and anything that needs collaborative management of mostly text files.\nAnother great and fun read about Git/GitHub and why it is a great tool for data analysis is in this article by Jenny Bryan."
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html#what-to-not-use-gitgithub-for",
    "href": "posts/2025-01-21-intro-git-github/index.html#what-to-not-use-gitgithub-for",
    "title": "Introduction to git/GitHub",
    "section": "What to (not) use Git/GitHub for",
    "text": "What to (not) use Git/GitHub for\nGitHub is ideal if you have a project with (possibly many) smallish files, and most of those files are text files (such as R code, LaTeX, Quarto/(R)Markdown, etc.) and different people work on different parts of the project.\nGitHub is less useful if you have a lot of non-text files (e.g. Word or PowerPoint) and different team members might want to edit the same document at the same time. In that instance, a solution like Google Docs, Word+Dropbox, Word+Onedrive, etc. might be better.\nGitHub also has a problem with large files. Anything above around 50MB can lead to very slow syncing and sometimes outright failure. Unfortunately, once GitHub has choked on a large file, it can be quite tricky to fix the repository to work again. This is because the ENTIRE history is saved, including the addition of the huge file. Therefore keep large (&gt;50MB) files out of your GitHub repositories. If you have to work with such files, try to reduce them first before placing into the GitHub repository. Or as alternative, place those files in another sync service (e.g. Dropbox, OneDrive, GoogleDrive) and load them from there.\nFinally, if you have data, you need to be careful since by default, GitHub repositories are public. You can set the repository to private, but you need to be careful that you don’t accidentally expose confidential data to the public. It is in general not a good idea to have confidential data on GitHub. First anonymize your data (ensure that it is not at risk of identifiability), then you can place it in a private repository. If you put it in a public repo, be very careful!! (And you may need IRB approval, check with your institutional research office.)\n\n\n\n\n\n\nTip\n\n\n\n\nGit/GitHub has version control features like a turbo-charged version of “track changes” but more rigorous, powerful, and scaled up to multiple files\nGreat for solo or collaborative work\nSaves the entire history of every change made\nAllows for multiple verisions or “branches” to be developed and later merged\nGitHub allows distributed collaboration (potentially among complete strangers) and has greatly promoted open-source software development, collaboration, distribution, and bug/issue tracking for users to get help\nGitHub allows webpages for your projects or repositories\n\n\n\nNote that other interfaces to Git exist, e.g., Bitbucket, but GitHub is the most widely used one."
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html#why-use-gitgithub",
    "href": "posts/2025-01-21-intro-git-github/index.html#why-use-gitgithub",
    "title": "Introduction to git/GitHub",
    "section": "Why use git/GitHub?",
    "text": "Why use git/GitHub?\nYou want to use GitHub to avoid this:\n\n\n\n\nHow not to use GitHub image from PhD Comics\n\n\n\nTo learn a bit more about Git/GitHub and why you might want to use it, read this article by Jenny Bryan.\nNote her explanation of what’s special with the README.md file on GitHub."
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html#how-to-use-gitgithub",
    "href": "posts/2025-01-21-intro-git-github/index.html#how-to-use-gitgithub",
    "title": "Introduction to git/GitHub",
    "section": "How to use Git/GitHub",
    "text": "How to use Git/GitHub\nGit/GitHub is fundamentally based on commands you type into the command line. Lots of online resources show you how to use the command line. This is the most powerful, and the way I almost always interact with git/GitHub. However, many folks find this the most confusing way to use git/GitHub. Alternatively, there are graphical interfaces.\n\n\nGitHub itself provides a grapical interface with basic functionality.\nRStudio also has Git/GitHub integration. Of course this only works for R project GitHub integration.\nThere are also third party GitHub clients with many advanced features, most of which you won’t need initially, but might eventually.\n\nNote: As student, you can (and should) upgrade to the Pro version of GitHub for free (i.e. access to unlimited private repositories is one benefit), see the GitHub student developer pack on how to do this."
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html#sec-profile",
    "href": "posts/2025-01-21-intro-git-github/index.html#sec-profile",
    "title": "Introduction to git/GitHub",
    "section": "Set up your profile in git on your computer",
    "text": "Set up your profile in git on your computer\nBefore making changes to your repository, GitHub will want to verify your identity.\nIn order for your computer to talk to GitHub smoothly, you will need to set up your username and email in the git profile stored on your computer.\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to match your GitHub account username and email! Otherwise GitHub wonʻt know who you are\n\n\n\n\nTerminal\n\ngit config --global user.name 'GitHubUsername'\ngit config --global user.email 'GitHub_email@example.com'\ngit config --global --list\n\nThat last line will show all of your current git config settings.\nIf you are using Rstudio, easy directions are provided here https://happygitwithr.com/hello-git.html"
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html#sec-token",
    "href": "posts/2025-01-21-intro-git-github/index.html#sec-token",
    "title": "Introduction to git/GitHub",
    "section": "Set up your Personal Authentication Token on your computer",
    "text": "Set up your Personal Authentication Token on your computer\nGitHub will also want to check your credentials to authenticate you are really you before writing changes to your repo.\nThere are several ways to do this, but the easiest is the protocol for HTTP authentication. You will generate a Personal Access Token for HTTPS from your GitHub account which will be stored on your personal machine.\nI prefer the GithHub command line interface or gh to do this. To install the CLI, follow the instructions here for your operating system. For Mac users, I suggest that you install homebrew, it is a command-line general software manager for many different software packages.\n\n\n\n\n\n\nStoring your Personal Access Token\n\n\n\n\nFrom GitHub: Generate your personal access token instructions\n\nFind the token generator on GitHub under your User Icon &gt; settings &gt; developer settings (left side bar) &gt; Personal access tokens &gt; Tokens (classic) &gt; Generate new Token &gt; classic note this menu may change\n\nSelect at least these scopes: “admin:org”,“repo”, “user”, “gist”, and “workflow”\n\n\nFrom your Command Line: Use gh auth login to store your token and follow the prompts.\n\nselect HTTPS for your preferred protocol\nselect Y for authenticate with GitHub credentials\nAlternatively if you want to do this all from the command line you can run the following line (if your token is saved in mytoken.txt):\n\n\n\nTerminal\n\ngh auth login --with-token &lt; mytoken.txt\n\n\n\n\n\nMany more details are explained nicely here https://happygitwithr.com/https-pat.html\nYou only have to store your credentials once for each computer (or PAT expiration date), then you can push and pull from GitHub to your heartʻs content. It really is a nice way to do things securely."
  },
  {
    "objectID": "posts/2025-01-21-intro-git-github/index.html#configuring-your-default-git-editor",
    "href": "posts/2025-01-21-intro-git-github/index.html#configuring-your-default-git-editor",
    "title": "Introduction to git/GitHub",
    "section": "Configuring your default git editor",
    "text": "Configuring your default git editor\nYou may want to set your default git editor to something you know how to use (it will come up when you have a merge conflict).\nFor example the nano editor is easy to use on the command line for a Unix shell:\n\n\nMac/Linux\n\ngit config --global core.editor \"nano -w\"\n\nThe Carpentries provide a full list of editors by operating system, a great resource."
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#the-modern-computationally-literate-scientist",
    "href": "posts/2025-01-14-the-big-picture/index.html#the-modern-computationally-literate-scientist",
    "title": "Introduction and The Big Idea",
    "section": "The Modern Computationally-Literate Scientist",
    "text": "The Modern Computationally-Literate Scientist\n\n\nUses computational tools to test ideas\nHas Computing Skills to:\n\nHandle any kind of data\nImplement any kind of test\nProduce graphics for exploration and communication\nTest and validate code (how do we know its right?)\nInteract with other computing systems and the Cloud\nCan archive and disseminate data and workflow\nProduce reproducible results!"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#data-science-workflow",
    "href": "posts/2025-01-14-the-big-picture/index.html#data-science-workflow",
    "title": "Introduction and The Big Idea",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nFrom R for Data Science 2e by Hadley Wickam, Garrett Grolemund, and Mine Çetinkaya-Rundel\n\n\n\nQuestion Development\nExploration and Testing\nCommunication"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#our-work-is-interdisciplinary",
    "href": "posts/2025-01-14-the-big-picture/index.html#our-work-is-interdisciplinary",
    "title": "Introduction and The Big Idea",
    "section": "Our Work is Interdisciplinary",
    "text": "Our Work is Interdisciplinary\n\n\nDisciplinary Knowledge (Biology) -&gt; Question Development\nStatistics -&gt; Exploration and Testing\nComputer Science -&gt; Repeatable, Scalable, Reusable\n\nCode MUST be FREE OF ERROR\nClean and Well documented (understandable)\nModular - enhances creativity and scalability"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#classwork-to-professional-science",
    "href": "posts/2025-01-14-the-big-picture/index.html#classwork-to-professional-science",
    "title": "Introduction and The Big Idea",
    "section": "Classwork to Professional Science",
    "text": "Classwork to Professional Science\nThere is a difference between one-time “getting it to work” vs. professional science (publication)\n\n\nThe “answer” must be correct - code validation\nMust be repeatable\nWorkflow must be complete, well organized, documented\nData and code shared on a public repository with a DOI\n\ne.g., GitHub and Zotero (free options!)\nor Dryad ($150 per dataset)"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#how-the-tools-fit-together",
    "href": "posts/2025-01-14-the-big-picture/index.html#how-the-tools-fit-together",
    "title": "Introduction and The Big Idea",
    "section": "How the Tools Fit Together",
    "text": "How the Tools Fit Together\n\n\n\n\n\nNeed\nTools\n\n\n\n\nObserve -&gt; Record Data -&gt; Data Table\nNotebooks\n\n\nCode -&gt; Document -&gt; Comment (annotate)\nR\n\n\nOrganize Project -&gt; Version Control -&gt; Share\nGit/GitHub\n\n\nCommunicate\nQuarto/Rmarkdown"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#open-source-tools-you-will-learn-in-this-course",
    "href": "posts/2025-01-14-the-big-picture/index.html#open-source-tools-you-will-learn-in-this-course",
    "title": "Introduction and The Big Idea",
    "section": "Open Source Tools you will learn in this course",
    "text": "Open Source Tools you will learn in this course\n\n\nR [watch short podcast, up to 4:00]\nCommand Line (UNIX/ Windows/ File Commands in your operating system)\n\nUnderstand your computer’s organization and filepaths\nMore precisely control your files and information\nInstall some command-line software\n\nGit/GitHub [course repo]\nQuarto/Rmarkdown [scientific and technical publishing system]\nand many more…"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#how-to-succeed",
    "href": "posts/2025-01-14-the-big-picture/index.html#how-to-succeed",
    "title": "Introduction and The Big Idea",
    "section": "How to Succeed",
    "text": "How to Succeed\n\n\nPractice\nMake errors – figure out how to fix them\nFearlessly ask questions\nTrial and Error is critical to learning\nValidate – check that the answer is right\nWhen you are developing a script, go back and clean it up!\nSave the correct, good code, throw out the mistakes\nDocument so that you can understand it 1 year from now"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#course-topics",
    "href": "posts/2025-01-14-the-big-picture/index.html#course-topics",
    "title": "Introduction and The Big Idea",
    "section": "Course Topics",
    "text": "Course Topics\n\n\nYour Computer\n\nWhere information is stored - FILEPATHS\nYour OS (Operating System)\n\nGit/GitHub\nR\nMaking them talk to each other\nCoding Fundamentals\nTour of Univariate + Multivariate Statistics\nGraphics\nSpecial Topics - Tell me your interests! [Google Form]"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#software",
    "href": "posts/2025-01-14-the-big-picture/index.html#software",
    "title": "Introduction and The Big Idea",
    "section": "Software",
    "text": "Software\n\nDo you have R installed and working? (R studio is optional)\nFor a longer walk-through here is another resource: Introduction to R/Rstudio\nGit/GitHub: Please Install for next time. Letʻs follow Introduction to Git/GitHub"
  },
  {
    "objectID": "posts/2025-01-14-the-big-picture/index.html#learning-r---a-first-session",
    "href": "posts/2025-01-14-the-big-picture/index.html#learning-r---a-first-session",
    "title": "Introduction and The Big Idea",
    "section": "Learning R - A first session",
    "text": "Learning R - A first session\n\n\nThink about how R works as you try out commands\n“Mistakes” are opportunities to learn how R works!\nLearning Language involves a lot of trial and error\nDon’t be afraid to try - poke it - it won’t break!\nhttps://www.r-project.org Go to manuals, click on An Introduction to R\nFollow Section 2.1\ninput -&gt; R -&gt; output\nWhat came out? What does it tell you about the rules R follows?\nComputers only do Exactly what you tell them to do\nJump to Appendix A - letʻs try to understand some rules of R together"
  },
  {
    "objectID": "projects/2023-03-22-oral-presentation/index.html",
    "href": "projects/2023-03-22-oral-presentation/index.html",
    "title": "Oral Presentation",
    "section": "",
    "text": "Oral Presentation\nOn the last day of class, we will have oral presentations.\nThey will be informal talks on your final projects. I want you to cover the following in about 10 minutes:\n\nBrief introduction to the disciplinary (biological, etc.) problem + More introduction to the data science (cleaning+analysis) problem.\n\nBriefly but Clearly describe your data, and itʻs structure\nWhat are you starting with? What are you aiming to get to?\n\nMethods - how did you do what you needed to do? Be sure to mention any new packages or tricky code you had to figure out. These often identify data manipulation steps that turn out to be critical.\nResults - what do the data show? What else did you find out?\nConclusions - what does it mean? What potential do you see here? If itʻs not finished, speculate on what you might be able to infer or what questions it might answer?\nWhat did you learn? What was most interesting, confusing, fun to figure out? Whatever you want to say\n\nSlides are optional, but can be helpful if you need a prompt to remember what to say. Or you can talk us through your writeup like I do in class, whatever is more comfortable for you. Do show us key elements like your repo, and how to access it, etc."
  },
  {
    "objectID": "projects/2023-03-22-project-2-3/index.html",
    "href": "projects/2023-03-22-project-2-3/index.html",
    "title": "Projects 2,3",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "projects/2023-03-22-project-2-3/index.html#acknowledgements",
    "href": "projects/2023-03-22-project-2-3/index.html#acknowledgements",
    "title": "Projects 2,3",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science in R for Biologists 2025",
    "section": "",
    "text": "Welcome to Data Science for Biologists at the University of Hawaiʻi!"
  },
  {
    "objectID": "index.html#what-is-this-course",
    "href": "index.html#what-is-this-course",
    "title": "Introduction to Data Science in R for Biologists 2025",
    "section": "What is this course?",
    "text": "What is this course?\nThis course covers the basics of computational and programming skills required for research in biological sciences and related disciplines. We will cover practical issues in data organization and management as well as programming in R and the tidyverse. Some of the topics will include: data ethics, best practices for coding and reproducible research, introduction to data visualizations, best practices for working with special data types (dates/times, text data, etc), best practices for storing data, basics of debugging, organizing and commenting code, basics of interacting with other computational resources from R. Topics in statistical data analysis, morphometrics, phylogenetic tree visualization, and other practical examples provide working examples."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Introduction to Data Science in R for Biologists 2025",
    "section": "Getting started",
    "text": "Getting started\nPlease look over the Syllabus and Schedule under General Information. Lectures are provided under the course materials tab."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Introduction to Data Science in R for Biologists 2025",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course was developed and is maintained by Marguerite Butler.\nA big thank you to Stephanie Hicks for generously sharing the beautifully designed quarto template for this course.\nMaterials have been adapted from courses developed by the following individuals (more to come): Stephanie Hicks.\nThe course materials are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "index.html#useful-free-r-resources",
    "href": "index.html#useful-free-r-resources",
    "title": "Introduction to Data Science in R for Biologists 2025",
    "section": "Useful (Free) R Resources",
    "text": "Useful (Free) R Resources\nIntro to R (by the R Core Group): https://cran.r-project.org/doc/manuals/r-release/R-intro.html R for Data Science: http://r4ds.had.co.nz/ Intro to Data Science: http://rafalab.dfci.harvard.edu/dsbook/ Various “Cheat Sheets”: https://www.rstudio.com/resources/cheatsheets/ DataCamp: http://www.datacamp.com R reference card: http://cran.r-project.org/doc/contrib/Short-refcard.pdfUCLA R Data Import/Export (by the R Core Group): https://cran.r-project.org/doc/manuals/r-release/R-data.html"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "A small tour of multivariate analysis\n\n\n\n\n\n\nmodule 4\n\n\nweek 8\n\n\nmultivariate\n\n\nstatistics\n\n\nR\n\n\n\nWhen you have relationships among many variables or want to identify group structure\n\n\n\n\n\nMar 11, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nA small review of univariate parametric statistics\n\n\n\n\n\n\nmodule 4\n\n\nweek 7\n\n\nunivariate\n\n\nstatistics\n\n\nggplot2\n\n\ndplyr\n\n\n\nWhat regression and ANOVA mean when exploring data\n\n\n\n\n\nMar 6, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nGetting data in shape with dplyr\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\ntidyr\n\n\ntidyverse\n\n\ndplyr\n\n\ntibble\n\n\npipe\n\n\n\nCrack that whip!\n\n\n\n\n\nFeb 27, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nThe ggplot2 package\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\nR\n\n\nprogramming\n\n\nplotting\n\n\nggplot2\n\n\ndata visualization\n\n\n\nIntroduction to the gplot2 grammar of graphics\n\n\n\n\n\nFeb 25, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nTidying and Exploring Data\n\n\n\n\n\n\nmodule 2\n\n\nweek 5\n\n\ndata\n\n\ndata structures\n\n\nobjects\n\n\n\nAll about data and how it is represented in R\n\n\n\n\n\nFeb 18, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nTypes of Data\n\n\n\n\n\n\nmodule 2\n\n\nweek 4\n\n\ndata\n\n\ndata structures\n\n\nobjects\n\n\n\nAll about data and how it is represented in R\n\n\n\n\n\nFeb 13, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nSaving your work as R scripts\n\n\n\n\n\n\nmodule 1\n\n\nweek 4\n\n\nR\n\n\nscripts\n\n\nreproducibility\n\n\n\nSave your hard work as scripts!\n\n\n\n\n\nFeb 11, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nData IO\n\n\n\n\n\n\nmodule 2\n\n\nweek 3\n\n\ndata\n\n\ninput\n\n\noutput\n\n\nformats\n\n\n\nSo many ways to get data into R\n\n\n\n\n\nFeb 4, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nReference management\n\n\n\n\n\n\nmodule 1\n\n\nweek 3\n\n\nQuarto\n\n\nauthoring\n\n\nBibTeX\n\n\nprogramming\n\n\n\nHow to use citations and include your bibliography in R Quarto.\n\n\n\n\n\nJan 30, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nLiterate Statistical Programming and Quarto\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nMarkdown\n\n\nQuarto\n\n\nprogramming\n\n\n\nIntroduction to literate statistical programming tools including Quarto Markdown\n\n\n\n\n\nJan 28, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to git/GitHub\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nprogramming\n\n\nversion control\n\n\ngit\n\n\nGitHub\n\n\n\nVersion control is a game changer; or how I learned to love git/GitHub\n\n\n\n\n\nJan 21, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to your computerʻs terminal utilities\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nprogramming\n\n\nfilesystem\n\n\nshell\n\n\n\nSo much power; or how I got my computer to do my bidding\n\n\n\n\n\nJan 16, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction and The Big Idea\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nintroduction\n\n\n\nThe big idea\n\n\n\n\n\nJan 14, 2025\n\n\nMarguerite Butler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Learning R\n\nBig Book of R: https://www.bigbookofr.com\nList of resources to learn R (but also Python, SQL, Javascript): https://github.com/delabj/datacamp_alternatives/blob/master/index.md\nlearnr4free. Resources (books, videos, interactive websites, papers) to learn R. Some of the resources are beginner-friendly and start with the installation process: https://www.learnr4free.com/en\nData Science with R by Danielle Navarro: https://robust-tools.djnavarro.net"
  }
]